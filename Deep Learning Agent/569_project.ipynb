{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaraVaseem/COMP569/blob/main/569_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GNN Model Implementation**\n"
      ],
      "metadata": {
        "id": "ZYgrxWIxbW0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#quicker installation\n",
        "!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuBYSqQszPVV",
        "outputId": "f8e62d6d-f78f-41ca-cb58-72f7dc25a86e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping torch-scatter as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-sparse as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-geometric as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-cluster as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_scatter-2.1.2%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt26cu124\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_sparse-0.6.18%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.14.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (2.0.2)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt26cu124\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_cluster-1.6.3%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-cluster) (1.14.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-cluster) (2.0.2)\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.3+pt26cu124\n",
            "Collecting git+https://github.com/pyg-team/pytorch_geometric.git\n",
            "  Cloning https://github.com/pyg-team/pytorch_geometric.git to /tmp/pip-req-build-4tnl95wz\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/pyg-team/pytorch_geometric.git /tmp/pip-req-build-4tnl95wz\n",
            "  Resolved https://github.com/pyg-team/pytorch_geometric.git to commit 628242c85b4be06cfa44f6bf83edae4797cfedd0\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (4.67.1)\n",
            "Collecting xxhash (from torch-geometric==2.7.0)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (1.19.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric==2.7.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.7.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.7.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.7.0) (2025.1.31)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.7.0-py3-none-any.whl size=1201463 sha256=291bfcf925bb96788899c3e075e1ed2093468251f90b1917f3dab876f0e71da1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qsuxni1i/wheels/93/bb/85/bfec4ee59b2563f74ec87cc2c91c6a4d3e40d3dcdec8ee5afe\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: xxhash, torch-geometric\n",
            "Successfully installed torch-geometric-2.7.0 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    import torch\n",
        "    import torch_geometric\n",
        "    from torch_geometric.data import Data\n",
        "    from torch_geometric.nn import GCNConv\n",
        "    import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "MAZdtEKbiR1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    edge_index = torch.tensor([[0, 1, 1, 2], [1, 0, 2, 1]], dtype=torch.long)\n",
        "    x = torch.tensor([[-1], [0], [1]], dtype=torch.float)\n",
        "    data = Data(x=x, edge_index=edge_index)"
      ],
      "metadata": {
        "id": "4ETR_MC7iazd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    class GCN(torch.nn.Module):\n",
        "        def __init__(self):\n",
        "            super(GCN, self).__init__()\n",
        "            self.conv1 = GCNConv(1, 16)\n",
        "            self.conv2 = GCNConv(16, 1)\n",
        "\n",
        "        def forward(self, data):\n",
        "            x, edge_index = data.x, data.edge_index\n",
        "            x = F.relu(self.conv1(x, edge_index))\n",
        "            x = self.conv2(x, edge_index)\n",
        "            return x\n",
        "\n",
        "    model = GCN()"
      ],
      "metadata": {
        "id": "wfqYVoLYicZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    for epoch in range(200):\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = F.mse_loss(out, torch.tensor([[-1.0], [1.0], [-1.0]], dtype=torch.float))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f'Epoch {epoch}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvBWxS4BiebJ",
        "outputId": "6775835a-76fc-4fbc-c61d-0897c754ae80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.0139325857162476\n",
            "Epoch 1, Loss: 0.9909705519676208\n",
            "Epoch 2, Loss: 0.9748308062553406\n",
            "Epoch 3, Loss: 0.9629521369934082\n",
            "Epoch 4, Loss: 0.9537804126739502\n",
            "Epoch 5, Loss: 0.9462776780128479\n",
            "Epoch 6, Loss: 0.9398136138916016\n",
            "Epoch 7, Loss: 0.9339094161987305\n",
            "Epoch 8, Loss: 0.9283655285835266\n",
            "Epoch 9, Loss: 0.9235706329345703\n",
            "Epoch 10, Loss: 0.9190990924835205\n",
            "Epoch 11, Loss: 0.914439857006073\n",
            "Epoch 12, Loss: 0.9097701907157898\n",
            "Epoch 13, Loss: 0.9051089286804199\n",
            "Epoch 14, Loss: 0.900468111038208\n",
            "Epoch 15, Loss: 0.8959066271781921\n",
            "Epoch 16, Loss: 0.8914053440093994\n",
            "Epoch 17, Loss: 0.886883556842804\n",
            "Epoch 18, Loss: 0.882239818572998\n",
            "Epoch 19, Loss: 0.8773913383483887\n",
            "Epoch 20, Loss: 0.8732545375823975\n",
            "Epoch 21, Loss: 0.8691956400871277\n",
            "Epoch 22, Loss: 0.8650640845298767\n",
            "Epoch 23, Loss: 0.8609123229980469\n",
            "Epoch 24, Loss: 0.856745719909668\n",
            "Epoch 25, Loss: 0.8529425263404846\n",
            "Epoch 26, Loss: 0.8497025966644287\n",
            "Epoch 27, Loss: 0.8460542559623718\n",
            "Epoch 28, Loss: 0.8420538902282715\n",
            "Epoch 29, Loss: 0.8378689885139465\n",
            "Epoch 30, Loss: 0.8335719108581543\n",
            "Epoch 31, Loss: 0.829612672328949\n",
            "Epoch 32, Loss: 0.8255460858345032\n",
            "Epoch 33, Loss: 0.8211495280265808\n",
            "Epoch 34, Loss: 0.8169484734535217\n",
            "Epoch 35, Loss: 0.8132045865058899\n",
            "Epoch 36, Loss: 0.8086672425270081\n",
            "Epoch 37, Loss: 0.803581714630127\n",
            "Epoch 38, Loss: 0.7990760207176208\n",
            "Epoch 39, Loss: 0.7945317625999451\n",
            "Epoch 40, Loss: 0.7897136211395264\n",
            "Epoch 41, Loss: 0.7847075462341309\n",
            "Epoch 42, Loss: 0.7799181938171387\n",
            "Epoch 43, Loss: 0.7751035690307617\n",
            "Epoch 44, Loss: 0.7699348330497742\n",
            "Epoch 45, Loss: 0.7649767994880676\n",
            "Epoch 46, Loss: 0.7600705027580261\n",
            "Epoch 47, Loss: 0.7549503445625305\n",
            "Epoch 48, Loss: 0.7493877410888672\n",
            "Epoch 49, Loss: 0.7446097731590271\n",
            "Epoch 50, Loss: 0.7396662831306458\n",
            "Epoch 51, Loss: 0.7341970801353455\n",
            "Epoch 52, Loss: 0.7282490730285645\n",
            "Epoch 53, Loss: 0.7232639789581299\n",
            "Epoch 54, Loss: 0.7183091044425964\n",
            "Epoch 55, Loss: 0.712857186794281\n",
            "Epoch 56, Loss: 0.7070146203041077\n",
            "Epoch 57, Loss: 0.7010242938995361\n",
            "Epoch 58, Loss: 0.695073127746582\n",
            "Epoch 59, Loss: 0.689567506313324\n",
            "Epoch 60, Loss: 0.6841195225715637\n",
            "Epoch 61, Loss: 0.678135871887207\n",
            "Epoch 62, Loss: 0.6719018816947937\n",
            "Epoch 63, Loss: 0.6655808091163635\n",
            "Epoch 64, Loss: 0.6597115397453308\n",
            "Epoch 65, Loss: 0.6536853313446045\n",
            "Epoch 66, Loss: 0.6472762227058411\n",
            "Epoch 67, Loss: 0.6410675048828125\n",
            "Epoch 68, Loss: 0.6351657509803772\n",
            "Epoch 69, Loss: 0.629010021686554\n",
            "Epoch 70, Loss: 0.6224873661994934\n",
            "Epoch 71, Loss: 0.6163670420646667\n",
            "Epoch 72, Loss: 0.6097955107688904\n",
            "Epoch 73, Loss: 0.6034238934516907\n",
            "Epoch 74, Loss: 0.5971541404724121\n",
            "Epoch 75, Loss: 0.5903860330581665\n",
            "Epoch 76, Loss: 0.5834975838661194\n",
            "Epoch 77, Loss: 0.5773999094963074\n",
            "Epoch 78, Loss: 0.571065366268158\n",
            "Epoch 79, Loss: 0.5645062327384949\n",
            "Epoch 80, Loss: 0.5577613711357117\n",
            "Epoch 81, Loss: 0.5508186221122742\n",
            "Epoch 82, Loss: 0.5440778136253357\n",
            "Epoch 83, Loss: 0.5374844074249268\n",
            "Epoch 84, Loss: 0.530826210975647\n",
            "Epoch 85, Loss: 0.5239446759223938\n",
            "Epoch 86, Loss: 0.5175430774688721\n",
            "Epoch 87, Loss: 0.5106319785118103\n",
            "Epoch 88, Loss: 0.5036205649375916\n",
            "Epoch 89, Loss: 0.4967400133609772\n",
            "Epoch 90, Loss: 0.4900945723056793\n",
            "Epoch 91, Loss: 0.48318541049957275\n",
            "Epoch 92, Loss: 0.47609391808509827\n",
            "Epoch 93, Loss: 0.4693070352077484\n",
            "Epoch 94, Loss: 0.46259188652038574\n",
            "Epoch 95, Loss: 0.45596596598625183\n",
            "Epoch 96, Loss: 0.4490170180797577\n",
            "Epoch 97, Loss: 0.44197049736976624\n",
            "Epoch 98, Loss: 0.43532004952430725\n",
            "Epoch 99, Loss: 0.42862746119499207\n",
            "Epoch 100, Loss: 0.42175522446632385\n",
            "Epoch 101, Loss: 0.41488826274871826\n",
            "Epoch 102, Loss: 0.40810152888298035\n",
            "Epoch 103, Loss: 0.4014187753200531\n",
            "Epoch 104, Loss: 0.39485594630241394\n",
            "Epoch 105, Loss: 0.387699693441391\n",
            "Epoch 106, Loss: 0.38085564970970154\n",
            "Epoch 107, Loss: 0.3743426501750946\n",
            "Epoch 108, Loss: 0.36789393424987793\n",
            "Epoch 109, Loss: 0.3610602915287018\n",
            "Epoch 110, Loss: 0.35415616631507874\n",
            "Epoch 111, Loss: 0.3478511571884155\n",
            "Epoch 112, Loss: 0.34159186482429504\n",
            "Epoch 113, Loss: 0.33515509963035583\n",
            "Epoch 114, Loss: 0.3282046616077423\n",
            "Epoch 115, Loss: 0.3219883441925049\n",
            "Epoch 116, Loss: 0.31599709391593933\n",
            "Epoch 117, Loss: 0.30961766839027405\n",
            "Epoch 118, Loss: 0.30328628420829773\n",
            "Epoch 119, Loss: 0.2970161437988281\n",
            "Epoch 120, Loss: 0.2907593250274658\n",
            "Epoch 121, Loss: 0.2842789590358734\n",
            "Epoch 122, Loss: 0.27804216742515564\n",
            "Epoch 123, Loss: 0.2720079720020294\n",
            "Epoch 124, Loss: 0.26619598269462585\n",
            "Epoch 125, Loss: 0.26034265756607056\n",
            "Epoch 126, Loss: 0.25431951880455017\n",
            "Epoch 127, Loss: 0.24838697910308838\n",
            "Epoch 128, Loss: 0.24254684150218964\n",
            "Epoch 129, Loss: 0.23679591715335846\n",
            "Epoch 130, Loss: 0.231483593583107\n",
            "Epoch 131, Loss: 0.22617584466934204\n",
            "Epoch 132, Loss: 0.22076310217380524\n",
            "Epoch 133, Loss: 0.21498186886310577\n",
            "Epoch 134, Loss: 0.20913858711719513\n",
            "Epoch 135, Loss: 0.2042522430419922\n",
            "Epoch 136, Loss: 0.19923843443393707\n",
            "Epoch 137, Loss: 0.19397906959056854\n",
            "Epoch 138, Loss: 0.18860794603824615\n",
            "Epoch 139, Loss: 0.18306566774845123\n",
            "Epoch 140, Loss: 0.17888088524341583\n",
            "Epoch 141, Loss: 0.1739676147699356\n",
            "Epoch 142, Loss: 0.16886985301971436\n",
            "Epoch 143, Loss: 0.16428661346435547\n",
            "Epoch 144, Loss: 0.15972210466861725\n",
            "Epoch 145, Loss: 0.1551307886838913\n",
            "Epoch 146, Loss: 0.15045540034770966\n",
            "Epoch 147, Loss: 0.14597736299037933\n",
            "Epoch 148, Loss: 0.14176726341247559\n",
            "Epoch 149, Loss: 0.13765205442905426\n",
            "Epoch 150, Loss: 0.1332707554101944\n",
            "Epoch 151, Loss: 0.12910854816436768\n",
            "Epoch 152, Loss: 0.12489467114210129\n",
            "Epoch 153, Loss: 0.12118169665336609\n",
            "Epoch 154, Loss: 0.11748268455266953\n",
            "Epoch 155, Loss: 0.1134631335735321\n",
            "Epoch 156, Loss: 0.10959235578775406\n",
            "Epoch 157, Loss: 0.10587403923273087\n",
            "Epoch 158, Loss: 0.10219964385032654\n",
            "Epoch 159, Loss: 0.0989692434668541\n",
            "Epoch 160, Loss: 0.09564099460840225\n",
            "Epoch 161, Loss: 0.09217920154333115\n",
            "Epoch 162, Loss: 0.08871958404779434\n",
            "Epoch 163, Loss: 0.08543320745229721\n",
            "Epoch 164, Loss: 0.08246412128210068\n",
            "Epoch 165, Loss: 0.07957310229539871\n",
            "Epoch 166, Loss: 0.07661852985620499\n",
            "Epoch 167, Loss: 0.07376080751419067\n",
            "Epoch 168, Loss: 0.0707816630601883\n",
            "Epoch 169, Loss: 0.06799836456775665\n",
            "Epoch 170, Loss: 0.06554383784532547\n",
            "Epoch 171, Loss: 0.0630602166056633\n",
            "Epoch 172, Loss: 0.06048073247075081\n",
            "Epoch 173, Loss: 0.05798408016562462\n",
            "Epoch 174, Loss: 0.05564029514789581\n",
            "Epoch 175, Loss: 0.05341312661767006\n",
            "Epoch 176, Loss: 0.051150161772966385\n",
            "Epoch 177, Loss: 0.048825666308403015\n",
            "Epoch 178, Loss: 0.04697437211871147\n",
            "Epoch 179, Loss: 0.044912245124578476\n",
            "Epoch 180, Loss: 0.043071549385786057\n",
            "Epoch 181, Loss: 0.041066862642765045\n",
            "Epoch 182, Loss: 0.03928155079483986\n",
            "Epoch 183, Loss: 0.037413667887449265\n",
            "Epoch 184, Loss: 0.035806670784950256\n",
            "Epoch 185, Loss: 0.03423111140727997\n",
            "Epoch 186, Loss: 0.03259919956326485\n",
            "Epoch 187, Loss: 0.031064683571457863\n",
            "Epoch 188, Loss: 0.029557595029473305\n",
            "Epoch 189, Loss: 0.02822612226009369\n",
            "Epoch 190, Loss: 0.026861680671572685\n",
            "Epoch 191, Loss: 0.02551165781915188\n",
            "Epoch 192, Loss: 0.024301394820213318\n",
            "Epoch 193, Loss: 0.023036248981952667\n",
            "Epoch 194, Loss: 0.02194119244813919\n",
            "Epoch 195, Loss: 0.020855508744716644\n",
            "Epoch 196, Loss: 0.01975375972688198\n",
            "Epoch 197, Loss: 0.018707996234297752\n",
            "Epoch 198, Loss: 0.01770300231873989\n",
            "Epoch 199, Loss: 0.01674707420170307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Ground truth labels\n",
        "y_true = torch.tensor([[-1.0], [1.0], [-1.0]], dtype=torch.float)"
      ],
      "metadata": {
        "id": "O3njkohjjRgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute accuracy after training\n",
        "with torch.no_grad():\n",
        "    predictions = model(data)\n",
        "\n",
        "    # Convert predictions to binary labels (-1 or 1)\n",
        "    predicted_labels = torch.where(predictions >= 0, torch.tensor([1.0]), torch.tensor([-1.0]))\n",
        "\n",
        "    # Compute accuracy\n",
        "    correct = (predicted_labels == y_true).sum().item()\n",
        "    accuracy = correct / y_true.numel()\n",
        "\n",
        "    print(f'Predicted labels:\\n{predicted_labels.numpy()}')\n",
        "    print(f'Ground truth labels:\\n{y_true.numpy()}')\n",
        "    print(f'Accuracy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcQCK20GjBy5",
        "outputId": "e5d5772c-f5c0-42a6-970a-596e1699a90a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted labels:\n",
            "[[-1.]\n",
            " [ 1.]\n",
            " [-1.]]\n",
            "Ground truth labels:\n",
            "[[-1.]\n",
            " [ 1.]\n",
            " [-1.]]\n",
            "Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is this? Training a GNN on 3 nodes (Small). Testing on the training set.\n"
      ],
      "metadata": {
        "id": "ztnx6PTzjxJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "torch.save(model.state_dict(), \"gcn_model.pth\")\n",
        "print(\"Model saved as gcn_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkHkslQ8kbDu",
        "outputId": "4b8367c5-3977-4d9f-f9e4-48ffa24ad857"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as gcn_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To load the model later\n",
        "model_loaded = GCN()\n",
        "model_loaded.load_state_dict(torch.load(\"gcn_model.pth\"))\n",
        "model_loaded.eval()  # Set the model to evaluation mode\n",
        "print(\"Model loaded successfully!\")"
      ],
      "metadata": {
        "id": "H8uqn-Hnkdm7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6f1272d-d813-4e98-94ee-fae098c8376e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset"
      ],
      "metadata": {
        "id": "9oBLLmrmt786"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import HeterophilousGraphDataset\n",
        "\n",
        "dataset = HeterophilousGraphDataset(root='.', name='Minesweeper')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeYAf63dwiSy",
        "outputId": "cbe1b497-0b99-4563-bc68-157a853821e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/yandex-research/heterophilous-graphs/raw/main/data/minesweeper.npz\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)\n",
        "print(\"number of graphs:\\t\\t\",len(dataset))\n",
        "print(\"number of classes:\\t\\t\",dataset.num_classes)\n",
        "print(\"number of node features:\\t\",dataset.num_node_features)\n",
        "print(\"number of edge features:\\t\",dataset.num_edge_features)\n",
        "print(\"class names:\\t\",dataset.classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "rvMNk6UO2Cfb",
        "outputId": "31b4b324-fc38-4295-be6f-9558b0dcc7f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HeterophilousGraphDataset(name=minesweeper)\n",
            "number of graphs:\t\t 1\n",
            "number of classes:\t\t 2\n",
            "number of node features:\t 7\n",
            "number of edge features:\t 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'HeterophilousGraphDataset' object has no attribute 'classes'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-524b21bb3851>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"number of node features:\\t\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_node_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"number of edge features:\\t\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_edge_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"class names:\\t\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/data/in_memory_dataset.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_data_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         raise AttributeError(f\"'{self.__class__.__name__}' object has no \"\n\u001b[0m\u001b[1;32m    319\u001b[0m                              f\"attribute '{key}'\")\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'HeterophilousGraphDataset' object has no attribute 'classes'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAIQCEJh2S-P",
        "outputId": "9dc6846a-432c-4cb9-ca0e-3feb0b207074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(x=[10000, 7], edge_index=[2, 78804], y=[10000], train_mask=[10000, 10], val_mask=[10000, 10], test_mask=[10000, 10])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"edge_index:\\t\\t\",dataset.data.edge_index.shape)\n",
        "print(dataset.data.edge_index)\n",
        "print(\"\\n\")\n",
        "print(\"train_mask:\\t\\t\",dataset.data.train_mask.shape)\n",
        "print(dataset.data.train_mask)\n",
        "print(\"\\n\")\n",
        "print(\"x:\\t\\t\",dataset.data.x.shape)\n",
        "print(dataset.data.x)\n",
        "print(\"\\n\")\n",
        "print(\"y:\\t\\t\",dataset.data.y.shape)\n",
        "print(dataset.data.y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Phq4Sd3g2-PP",
        "outputId": "b3e7945e-5425-4437-c658-6460a9f4dada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "edge_index:\t\t torch.Size([2, 78804])\n",
            "tensor([[   0,    0,    0,  ..., 9999, 9999, 9999],\n",
            "        [   1,  100,  101,  ..., 9898, 9899, 9998]])\n",
            "\n",
            "\n",
            "train_mask:\t\t torch.Size([10000, 10])\n",
            "tensor([[False, False, False,  ...,  True, False, False],\n",
            "        [False,  True,  True,  ...,  True, False,  True],\n",
            "        [ True,  True, False,  ..., False,  True, False],\n",
            "        ...,\n",
            "        [False, False, False,  ..., False, False,  True],\n",
            "        [ True,  True,  True,  ...,  True, False, False],\n",
            "        [ True,  True,  True,  ..., False, False,  True]])\n",
            "\n",
            "\n",
            "x:\t\t torch.Size([10000, 7])\n",
            "tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
            "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
            "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
            "\n",
            "\n",
            "y:\t\t torch.Size([10000])\n",
            "tensor([0, 1, 0,  ..., 0, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice"
      ],
      "metadata": {
        "id": "bJIwx4gsZNfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"y shape:\", dataset[0].y.shape)\n",
        "print(\"Sample y:\", dataset[0].y[:10])\n",
        "print(\"train_mask shape:\", dataset[0].train_mask.shape)\n",
        "print(\"Non-zero in train_mask:\", dataset[0].train_mask.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQnBQboikaK8",
        "outputId": "ddca6aba-0b35-4a36-df82-d6e5888e8e6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y shape: torch.Size([10000])\n",
            "Sample y: tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0])\n",
            "train_mask shape: torch.Size([10000, 10])\n",
            "Non-zero in train_mask: tensor(50000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    class GCN(torch.nn.Module):\n",
        "        def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "            super(GCN, self).__init__()\n",
        "            self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "            self.conv2 = GCNConv(hidden_channels, out_channels)\n",
        "\n",
        "        def forward(self, data):\n",
        "            x, edge_index = data.x, data.edge_index\n",
        "            x = F.relu(self.conv1(x, edge_index))\n",
        "            x = self.conv2(x, edge_index)\n",
        "            return x\n",
        "\n",
        "    data = dataset[0]\n",
        "    train_mask = data.train_mask.any(dim=1)\n",
        "\n",
        "    model = GCN(in_channels=dataset.num_node_features, hidden_channels=16, out_channels=dataset.num_classes)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    for epoch in range(200):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = F.cross_entropy(out[train_mask], data.y[train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f'Epoch {epoch}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DYs7faHlgOu",
        "outputId": "7ed27299-c95c-4588-86f0-66b478849ba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.6180638670921326\n",
            "Epoch 1, Loss: 0.589022159576416\n",
            "Epoch 2, Loss: 0.564655601978302\n",
            "Epoch 3, Loss: 0.5445550680160522\n",
            "Epoch 4, Loss: 0.5283856987953186\n",
            "Epoch 5, Loss: 0.515764594078064\n",
            "Epoch 6, Loss: 0.5064384937286377\n",
            "Epoch 7, Loss: 0.5001710057258606\n",
            "Epoch 8, Loss: 0.49670088291168213\n",
            "Epoch 9, Loss: 0.4955049157142639\n",
            "Epoch 10, Loss: 0.4958668649196625\n",
            "Epoch 11, Loss: 0.4969862401485443\n",
            "Epoch 12, Loss: 0.49819812178611755\n",
            "Epoch 13, Loss: 0.4990293085575104\n",
            "Epoch 14, Loss: 0.4992034137248993\n",
            "Epoch 15, Loss: 0.498667448759079\n",
            "Epoch 16, Loss: 0.49748408794403076\n",
            "Epoch 17, Loss: 0.49577224254608154\n",
            "Epoch 18, Loss: 0.49372634291648865\n",
            "Epoch 19, Loss: 0.4915218949317932\n",
            "Epoch 20, Loss: 0.4893347918987274\n",
            "Epoch 21, Loss: 0.487306147813797\n",
            "Epoch 22, Loss: 0.48553138971328735\n",
            "Epoch 23, Loss: 0.4840639531612396\n",
            "Epoch 24, Loss: 0.48291510343551636\n",
            "Epoch 25, Loss: 0.4820491671562195\n",
            "Epoch 26, Loss: 0.4814085364341736\n",
            "Epoch 27, Loss: 0.48091810941696167\n",
            "Epoch 28, Loss: 0.48049625754356384\n",
            "Epoch 29, Loss: 0.4800702631473541\n",
            "Epoch 30, Loss: 0.4795841872692108\n",
            "Epoch 31, Loss: 0.47899559140205383\n",
            "Epoch 32, Loss: 0.4782877266407013\n",
            "Epoch 33, Loss: 0.47745898365974426\n",
            "Epoch 34, Loss: 0.4765232801437378\n",
            "Epoch 35, Loss: 0.475509911775589\n",
            "Epoch 36, Loss: 0.4744546711444855\n",
            "Epoch 37, Loss: 0.4733887314796448\n",
            "Epoch 38, Loss: 0.4723392426967621\n",
            "Epoch 39, Loss: 0.4713270664215088\n",
            "Epoch 40, Loss: 0.47035861015319824\n",
            "Epoch 41, Loss: 0.46943292021751404\n",
            "Epoch 42, Loss: 0.4685358703136444\n",
            "Epoch 43, Loss: 0.46765175461769104\n",
            "Epoch 44, Loss: 0.4667598009109497\n",
            "Epoch 45, Loss: 0.46585601568222046\n",
            "Epoch 46, Loss: 0.4649399518966675\n",
            "Epoch 47, Loss: 0.4640166163444519\n",
            "Epoch 48, Loss: 0.4630879759788513\n",
            "Epoch 49, Loss: 0.4621427059173584\n",
            "Epoch 50, Loss: 0.46120980381965637\n",
            "Epoch 51, Loss: 0.4603455066680908\n",
            "Epoch 52, Loss: 0.45956718921661377\n",
            "Epoch 53, Loss: 0.4588604271411896\n",
            "Epoch 54, Loss: 0.45819878578186035\n",
            "Epoch 55, Loss: 0.4575504958629608\n",
            "Epoch 56, Loss: 0.4569077491760254\n",
            "Epoch 57, Loss: 0.4562745690345764\n",
            "Epoch 58, Loss: 0.455661416053772\n",
            "Epoch 59, Loss: 0.4551164209842682\n",
            "Epoch 60, Loss: 0.45468100905418396\n",
            "Epoch 61, Loss: 0.45436325669288635\n",
            "Epoch 62, Loss: 0.45413073897361755\n",
            "Epoch 63, Loss: 0.45392388105392456\n",
            "Epoch 64, Loss: 0.4536975026130676\n",
            "Epoch 65, Loss: 0.4534561336040497\n",
            "Epoch 66, Loss: 0.45323169231414795\n",
            "Epoch 67, Loss: 0.45305079221725464\n",
            "Epoch 68, Loss: 0.4529184103012085\n",
            "Epoch 69, Loss: 0.45279818773269653\n",
            "Epoch 70, Loss: 0.45265328884124756\n",
            "Epoch 71, Loss: 0.45247721672058105\n",
            "Epoch 72, Loss: 0.4522925615310669\n",
            "Epoch 73, Loss: 0.4521249830722809\n",
            "Epoch 74, Loss: 0.451980322599411\n",
            "Epoch 75, Loss: 0.4518425762653351\n",
            "Epoch 76, Loss: 0.45169126987457275\n",
            "Epoch 77, Loss: 0.4515231251716614\n",
            "Epoch 78, Loss: 0.45135560631752014\n",
            "Epoch 79, Loss: 0.4512054920196533\n",
            "Epoch 80, Loss: 0.4510744512081146\n",
            "Epoch 81, Loss: 0.4509497880935669\n",
            "Epoch 82, Loss: 0.45082053542137146\n",
            "Epoch 83, Loss: 0.4506888687610626\n",
            "Epoch 84, Loss: 0.4505663514137268\n",
            "Epoch 85, Loss: 0.450460284948349\n",
            "Epoch 86, Loss: 0.45036613941192627\n",
            "Epoch 87, Loss: 0.45027434825897217\n",
            "Epoch 88, Loss: 0.4501813054084778\n",
            "Epoch 89, Loss: 0.4500928223133087\n",
            "Epoch 90, Loss: 0.45001524686813354\n",
            "Epoch 91, Loss: 0.4499480128288269\n",
            "Epoch 92, Loss: 0.4498846232891083\n",
            "Epoch 93, Loss: 0.4498205780982971\n",
            "Epoch 94, Loss: 0.44975799322128296\n",
            "Epoch 95, Loss: 0.4497014582157135\n",
            "Epoch 96, Loss: 0.4496520757675171\n",
            "Epoch 97, Loss: 0.4496056139469147\n",
            "Epoch 98, Loss: 0.44955870509147644\n",
            "Epoch 99, Loss: 0.4495125114917755\n",
            "Epoch 100, Loss: 0.449470579624176\n",
            "Epoch 101, Loss: 0.4494330585002899\n",
            "Epoch 102, Loss: 0.4493972063064575\n",
            "Epoch 103, Loss: 0.4493612051010132\n",
            "Epoch 104, Loss: 0.44932612776756287\n",
            "Epoch 105, Loss: 0.44929447770118713\n",
            "Epoch 106, Loss: 0.4492660462856293\n",
            "Epoch 107, Loss: 0.44923853874206543\n",
            "Epoch 108, Loss: 0.44921156764030457\n",
            "Epoch 109, Loss: 0.4491868019104004\n",
            "Epoch 110, Loss: 0.4491652548313141\n",
            "Epoch 111, Loss: 0.44914600253105164\n",
            "Epoch 112, Loss: 0.4491274058818817\n",
            "Epoch 113, Loss: 0.44910940527915955\n",
            "Epoch 114, Loss: 0.4490934908390045\n",
            "Epoch 115, Loss: 0.4490794837474823\n",
            "Epoch 116, Loss: 0.4490662217140198\n",
            "Epoch 117, Loss: 0.4490530490875244\n",
            "Epoch 118, Loss: 0.4490404725074768\n",
            "Epoch 119, Loss: 0.4490291476249695\n",
            "Epoch 120, Loss: 0.44901859760284424\n",
            "Epoch 121, Loss: 0.449008047580719\n",
            "Epoch 122, Loss: 0.44899770617485046\n",
            "Epoch 123, Loss: 0.44898781180381775\n",
            "Epoch 124, Loss: 0.44897839426994324\n",
            "Epoch 125, Loss: 0.44896891713142395\n",
            "Epoch 126, Loss: 0.44895949959754944\n",
            "Epoch 127, Loss: 0.4489504396915436\n",
            "Epoch 128, Loss: 0.44894155859947205\n",
            "Epoch 129, Loss: 0.4489325284957886\n",
            "Epoch 130, Loss: 0.4489234387874603\n",
            "Epoch 131, Loss: 0.4489145576953888\n",
            "Epoch 132, Loss: 0.4489060044288635\n",
            "Epoch 133, Loss: 0.44889745116233826\n",
            "Epoch 134, Loss: 0.4488888084888458\n",
            "Epoch 135, Loss: 0.4488804042339325\n",
            "Epoch 136, Loss: 0.44887202978134155\n",
            "Epoch 137, Loss: 0.448863685131073\n",
            "Epoch 138, Loss: 0.44885528087615967\n",
            "Epoch 139, Loss: 0.4488467872142792\n",
            "Epoch 140, Loss: 0.4488384425640106\n",
            "Epoch 141, Loss: 0.4488299787044525\n",
            "Epoch 142, Loss: 0.4488213360309601\n",
            "Epoch 143, Loss: 0.4488126039505005\n",
            "Epoch 144, Loss: 0.44880369305610657\n",
            "Epoch 145, Loss: 0.4487946629524231\n",
            "Epoch 146, Loss: 0.4487855136394501\n",
            "Epoch 147, Loss: 0.44877639412879944\n",
            "Epoch 148, Loss: 0.4487670958042145\n",
            "Epoch 149, Loss: 0.44875767827033997\n",
            "Epoch 150, Loss: 0.4487482011318207\n",
            "Epoch 151, Loss: 0.4487389326095581\n",
            "Epoch 152, Loss: 0.4487296938896179\n",
            "Epoch 153, Loss: 0.44872036576271057\n",
            "Epoch 154, Loss: 0.44871097803115845\n",
            "Epoch 155, Loss: 0.4487014412879944\n",
            "Epoch 156, Loss: 0.44869187474250793\n",
            "Epoch 157, Loss: 0.44868239760398865\n",
            "Epoch 158, Loss: 0.44867271184921265\n",
            "Epoch 159, Loss: 0.4486631155014038\n",
            "Epoch 160, Loss: 0.44865351915359497\n",
            "Epoch 161, Loss: 0.4486441910266876\n",
            "Epoch 162, Loss: 0.44863471388816833\n",
            "Epoch 163, Loss: 0.4486255943775177\n",
            "Epoch 164, Loss: 0.44861680269241333\n",
            "Epoch 165, Loss: 0.4486079812049866\n",
            "Epoch 166, Loss: 0.44859933853149414\n",
            "Epoch 167, Loss: 0.4485909044742584\n",
            "Epoch 168, Loss: 0.4485825300216675\n",
            "Epoch 169, Loss: 0.44857412576675415\n",
            "Epoch 170, Loss: 0.4485657811164856\n",
            "Epoch 171, Loss: 0.4485578238964081\n",
            "Epoch 172, Loss: 0.44854986667633057\n",
            "Epoch 173, Loss: 0.4485418498516083\n",
            "Epoch 174, Loss: 0.4485337436199188\n",
            "Epoch 175, Loss: 0.44852566719055176\n",
            "Epoch 176, Loss: 0.4485175609588623\n",
            "Epoch 177, Loss: 0.4485096037387848\n",
            "Epoch 178, Loss: 0.4485016465187073\n",
            "Epoch 179, Loss: 0.44849368929862976\n",
            "Epoch 180, Loss: 0.4484855830669403\n",
            "Epoch 181, Loss: 0.44847774505615234\n",
            "Epoch 182, Loss: 0.4484698474407196\n",
            "Epoch 183, Loss: 0.4484620690345764\n",
            "Epoch 184, Loss: 0.448454350233078\n",
            "Epoch 185, Loss: 0.4484465420246124\n",
            "Epoch 186, Loss: 0.44843876361846924\n",
            "Epoch 187, Loss: 0.44843095541000366\n",
            "Epoch 188, Loss: 0.44842323660850525\n",
            "Epoch 189, Loss: 0.44841551780700684\n",
            "Epoch 190, Loss: 0.44840773940086365\n",
            "Epoch 191, Loss: 0.44839993119239807\n",
            "Epoch 192, Loss: 0.4483921229839325\n",
            "Epoch 193, Loss: 0.4483844041824341\n",
            "Epoch 194, Loss: 0.44837671518325806\n",
            "Epoch 195, Loss: 0.44836896657943726\n",
            "Epoch 196, Loss: 0.44836124777793884\n",
            "Epoch 197, Loss: 0.4483535587787628\n",
            "Epoch 198, Loss: 0.44834595918655396\n",
            "Epoch 199, Loss: 0.4483383297920227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "data = dataset[0]\n",
        "test_mask = data.test_mask.any(dim=1)\n",
        "out = model(data)\n",
        "pred = out.argmax(dim=1)\n",
        "\n",
        "correct = (pred[test_mask] == data.y[test_mask]).sum()\n",
        "accuracy = int(correct) / int(test_mask.sum())\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XS-spWhFmHnk",
        "outputId": "137b4b71-11ac-4ac5-d587-98fd27fdc697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 80.22%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    from torch_geometric.nn import GCNConv, BatchNorm\n",
        "    import torch.nn.functional as F\n",
        "    from torch_geometric.nn import GATConv"
      ],
      "metadata": {
        "id": "WMEtPPUFpWat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.nn.functional as F\n",
        "    from torch_sparse import spmm\n",
        "    from torch_geometric.utils import add_self_loops, degree\n",
        "\n",
        "    class CustomDeepGPRGNN(nn.Module):\n",
        "        def __init__(self, in_channels, hidden_channels, out_channels, K=10, dropout=0.6, temperature=1.5):\n",
        "            super(CustomDeepGPRGNN, self).__init__()\n",
        "            self.K = K\n",
        "            self.dropout = dropout\n",
        "            self.temperature = temperature\n",
        "\n",
        "            self.fc1 = nn.Linear(in_channels, hidden_channels)\n",
        "            self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
        "\n",
        "            self.fc2 = nn.Linear(hidden_channels, hidden_channels)\n",
        "            self.bn2 = nn.BatchNorm1d(hidden_channels)\n",
        "\n",
        "            self.fc3 = nn.Linear(hidden_channels, hidden_channels)\n",
        "            self.bn3 = nn.BatchNorm1d(hidden_channels)\n",
        "\n",
        "            self.fc4 = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "            self.alpha = nn.Parameter(torch.Tensor(K + 1))\n",
        "            nn.init.constant_(self.alpha, 1.0 / (K + 1))\n",
        "\n",
        "        def forward(self, data):\n",
        "            x, edge_index = data.x, data.edge_index\n",
        "            edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
        "\n",
        "            row, col = edge_index\n",
        "            deg = degree(row, x.size(0), dtype=x.dtype)\n",
        "            deg_inv_sqrt = deg.pow(-0.5)\n",
        "            deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "            norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
        "\n",
        "\n",
        "            x0 = F.gelu(self.bn1(self.fc1(x)))\n",
        "            x1 = F.gelu(self.bn2(self.fc2(F.dropout(x0, p=self.dropout, training=self.training))) + x0)\n",
        "            x2 = F.gelu(self.bn3(self.fc3(F.dropout(x1, p=self.dropout, training=self.training))) + x1)\n",
        "            x = self.fc4(F.dropout(x2, p=self.dropout, training=self.training))\n",
        "\n",
        "\n",
        "            x_prop = self.alpha[0] * x\n",
        "            x_temp = x\n",
        "            for k in range(1, self.K + 1):\n",
        "                x_temp = spmm(edge_index, norm, x.size(0), x.size(0), x_temp)\n",
        "                x_prop += self.alpha[k] * x_temp\n",
        "\n",
        "            return x_prop / self.temperature\n"
      ],
      "metadata": {
        "id": "yitQ2kF5jolE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0, alpha=None):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        log_probs = F.log_softmax(inputs, dim=1)\n",
        "        probs = torch.exp(log_probs)\n",
        "        targets_one_hot = F.one_hot(targets, num_classes=inputs.size(1)).float()\n",
        "\n",
        "        focal_weight = (1 - probs) ** self.gamma\n",
        "        loss = -focal_weight * log_probs * targets_one_hot\n",
        "\n",
        "        if self.alpha is not None:\n",
        "            alpha = self.alpha[targets].unsqueeze(1)\n",
        "            loss = loss * alpha\n",
        "\n",
        "        return loss.sum(dim=1).mean()\n"
      ],
      "metadata": {
        "id": "w9uCTA3Ct1an"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    model = CustomDeepGPRGNN(\n",
        "        in_channels=dataset.num_node_features,\n",
        "        hidden_channels=128,\n",
        "        out_channels=dataset.num_classes,\n",
        "        K=20,\n",
        "        dropout=0.2\n",
        "    )\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "    criterion = FocalLoss(gamma=2.0, alpha=torch.tensor([1.0, 1.5]).to(data.x.device))\n",
        "\n",
        "    for epoch in range(150):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(dataset[0])\n",
        "        train_mask = dataset[0].train_mask.any(dim=1)\n",
        "        loss = criterion(out[train_mask], dataset[0].y[train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inpIUvUZj9pL",
        "outputId": "638d307c-70e9-4667-a975-9e3acfb8837e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.1665\n",
            "Epoch 1, Loss: 0.4276\n",
            "Epoch 2, Loss: 0.2100\n",
            "Epoch 3, Loss: 0.2212\n",
            "Epoch 4, Loss: 0.1704\n",
            "Epoch 5, Loss: 0.1564\n",
            "Epoch 6, Loss: 0.1575\n",
            "Epoch 7, Loss: 0.1590\n",
            "Epoch 8, Loss: 0.1597\n",
            "Epoch 9, Loss: 0.1601\n",
            "Epoch 10, Loss: 0.1612\n",
            "Epoch 11, Loss: 0.1625\n",
            "Epoch 12, Loss: 0.1632\n",
            "Epoch 13, Loss: 0.1628\n",
            "Epoch 14, Loss: 0.1611\n",
            "Epoch 15, Loss: 0.1595\n",
            "Epoch 16, Loss: 0.1585\n",
            "Epoch 17, Loss: 0.1587\n",
            "Epoch 18, Loss: 0.1589\n",
            "Epoch 19, Loss: 0.1579\n",
            "Epoch 20, Loss: 0.1561\n",
            "Epoch 21, Loss: 0.1545\n",
            "Epoch 22, Loss: 0.1534\n",
            "Epoch 23, Loss: 0.1527\n",
            "Epoch 24, Loss: 0.1516\n",
            "Epoch 25, Loss: 0.1498\n",
            "Epoch 26, Loss: 0.1476\n",
            "Epoch 27, Loss: 0.1456\n",
            "Epoch 28, Loss: 0.1439\n",
            "Epoch 29, Loss: 0.1421\n",
            "Epoch 30, Loss: 0.1401\n",
            "Epoch 31, Loss: 0.1378\n",
            "Epoch 32, Loss: 0.1357\n",
            "Epoch 33, Loss: 0.1339\n",
            "Epoch 34, Loss: 0.1324\n",
            "Epoch 35, Loss: 0.1307\n",
            "Epoch 36, Loss: 0.1293\n",
            "Epoch 37, Loss: 0.1278\n",
            "Epoch 38, Loss: 0.1265\n",
            "Epoch 39, Loss: 0.1253\n",
            "Epoch 40, Loss: 0.1243\n",
            "Epoch 41, Loss: 0.1232\n",
            "Epoch 42, Loss: 0.1224\n",
            "Epoch 43, Loss: 0.1214\n",
            "Epoch 44, Loss: 0.1202\n",
            "Epoch 45, Loss: 0.1194\n",
            "Epoch 46, Loss: 0.1184\n",
            "Epoch 47, Loss: 0.1176\n",
            "Epoch 48, Loss: 0.1166\n",
            "Epoch 49, Loss: 0.1158\n",
            "Epoch 50, Loss: 0.1151\n",
            "Epoch 51, Loss: 0.1144\n",
            "Epoch 52, Loss: 0.1138\n",
            "Epoch 53, Loss: 0.1128\n",
            "Epoch 54, Loss: 0.1124\n",
            "Epoch 55, Loss: 0.1116\n",
            "Epoch 56, Loss: 0.1110\n",
            "Epoch 57, Loss: 0.1106\n",
            "Epoch 58, Loss: 0.1100\n",
            "Epoch 59, Loss: 0.1097\n",
            "Epoch 60, Loss: 0.1091\n",
            "Epoch 61, Loss: 0.1084\n",
            "Epoch 62, Loss: 0.1082\n",
            "Epoch 63, Loss: 0.1075\n",
            "Epoch 64, Loss: 0.1072\n",
            "Epoch 65, Loss: 0.1066\n",
            "Epoch 66, Loss: 0.1062\n",
            "Epoch 67, Loss: 0.1059\n",
            "Epoch 68, Loss: 0.1056\n",
            "Epoch 69, Loss: 0.1050\n",
            "Epoch 70, Loss: 0.1046\n",
            "Epoch 71, Loss: 0.1045\n",
            "Epoch 72, Loss: 0.1040\n",
            "Epoch 73, Loss: 0.1039\n",
            "Epoch 74, Loss: 0.1034\n",
            "Epoch 75, Loss: 0.1030\n",
            "Epoch 76, Loss: 0.1028\n",
            "Epoch 77, Loss: 0.1024\n",
            "Epoch 78, Loss: 0.1024\n",
            "Epoch 79, Loss: 0.1022\n",
            "Epoch 80, Loss: 0.1019\n",
            "Epoch 81, Loss: 0.1017\n",
            "Epoch 82, Loss: 0.1013\n",
            "Epoch 83, Loss: 0.1013\n",
            "Epoch 84, Loss: 0.1010\n",
            "Epoch 85, Loss: 0.1009\n",
            "Epoch 86, Loss: 0.1006\n",
            "Epoch 87, Loss: 0.1005\n",
            "Epoch 88, Loss: 0.1004\n",
            "Epoch 89, Loss: 0.1006\n",
            "Epoch 90, Loss: 0.1003\n",
            "Epoch 91, Loss: 0.1003\n",
            "Epoch 92, Loss: 0.1002\n",
            "Epoch 93, Loss: 0.1001\n",
            "Epoch 94, Loss: 0.0999\n",
            "Epoch 95, Loss: 0.1000\n",
            "Epoch 96, Loss: 0.0998\n",
            "Epoch 97, Loss: 0.1000\n",
            "Epoch 98, Loss: 0.0998\n",
            "Epoch 99, Loss: 0.1002\n",
            "Epoch 100, Loss: 0.1002\n",
            "Epoch 101, Loss: 0.0998\n",
            "Epoch 102, Loss: 0.0999\n",
            "Epoch 103, Loss: 0.0998\n",
            "Epoch 104, Loss: 0.0996\n",
            "Epoch 105, Loss: 0.0999\n",
            "Epoch 106, Loss: 0.0997\n",
            "Epoch 107, Loss: 0.1000\n",
            "Epoch 108, Loss: 0.0997\n",
            "Epoch 109, Loss: 0.0997\n",
            "Epoch 110, Loss: 0.0997\n",
            "Epoch 111, Loss: 0.0996\n",
            "Epoch 112, Loss: 0.0995\n",
            "Epoch 113, Loss: 0.0995\n",
            "Epoch 114, Loss: 0.0997\n",
            "Epoch 115, Loss: 0.0996\n",
            "Epoch 116, Loss: 0.0998\n",
            "Epoch 117, Loss: 0.0999\n",
            "Epoch 118, Loss: 0.0995\n",
            "Epoch 119, Loss: 0.0994\n",
            "Epoch 120, Loss: 0.0996\n",
            "Epoch 121, Loss: 0.0994\n",
            "Epoch 122, Loss: 0.0995\n",
            "Epoch 123, Loss: 0.0996\n",
            "Epoch 124, Loss: 0.1001\n",
            "Epoch 125, Loss: 0.1008\n",
            "Epoch 126, Loss: 0.1006\n",
            "Epoch 127, Loss: 0.0996\n",
            "Epoch 128, Loss: 0.0998\n",
            "Epoch 129, Loss: 0.0997\n",
            "Epoch 130, Loss: 0.0997\n",
            "Epoch 131, Loss: 0.0999\n",
            "Epoch 132, Loss: 0.0996\n",
            "Epoch 133, Loss: 0.0995\n",
            "Epoch 134, Loss: 0.0998\n",
            "Epoch 135, Loss: 0.0998\n",
            "Epoch 136, Loss: 0.0998\n",
            "Epoch 137, Loss: 0.0996\n",
            "Epoch 138, Loss: 0.0997\n",
            "Epoch 139, Loss: 0.0998\n",
            "Epoch 140, Loss: 0.0996\n",
            "Epoch 141, Loss: 0.0997\n",
            "Epoch 142, Loss: 0.0998\n",
            "Epoch 143, Loss: 0.0997\n",
            "Epoch 144, Loss: 0.0996\n",
            "Epoch 145, Loss: 0.0998\n",
            "Epoch 146, Loss: 0.0994\n",
            "Epoch 147, Loss: 0.0996\n",
            "Epoch 148, Loss: 0.0999\n",
            "Epoch 149, Loss: 0.0995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "data = dataset[0]\n",
        "test_mask = data.test_mask.any(dim=1)\n",
        "out = model(data)\n",
        "pred = out.argmax(dim=1)\n",
        "\n",
        "correct = (pred[test_mask] == data.y[test_mask]).sum()\n",
        "accuracy = int(correct) / int(test_mask.sum())\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IcG0Blnk0aC",
        "outputId": "4688e8b0-1b0a-48c1-ad5d-97400c9cef41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 84.97%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model.eval()\n",
        "data = dataset[0]\n",
        "test_mask = data.test_mask.any(dim=1)\n",
        "\n",
        "with torch.no_grad():\n",
        "    out = model(data)\n",
        "    preds = out[test_mask].argmax(dim=1).cpu().numpy()\n",
        "    labels = data.y[test_mask].cpu().numpy()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(labels, preds, digits=4))\n",
        "\n",
        "cm = confusion_matrix(labels, preds)\n",
        "plt.figure(figsize=(8, 6))\n",
        "class_labels = [str(i) for i in range(len(cm))]\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "id": "6xsFzMNlv67v",
        "outputId": "59a8349d-13a7-4676-a7c9-c9e8ec555ab2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8974    0.9171    0.9071      7560\n",
            "           1     0.6363    0.5804    0.6071      1890\n",
            "\n",
            "    accuracy                         0.8497      9450\n",
            "   macro avg     0.7668    0.7487    0.7571      9450\n",
            "weighted avg     0.8451    0.8497    0.8471      9450\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAApIAAAIjCAYAAACwHvu2AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAASBlJREFUeJzt3XlcVdX+//H3QeWAIiAqIDlhlkrOWkpOWSQVlqZWpiWa1tWLluDcYGomXcsxUzNLvKWVDVpqZaapmTiE4azXMTIFR0AcAOH8/vDH+XYSE1YeQc/reR/78Yi911l77fNI76f3WnthsdlsNgEAAACF5FbUAwAAAMCNiUISAAAARigkAQAAYIRCEgAAAEYoJAEAAGCEQhIAAABGKCQBAABghEISAAAARigkAQAAYIRCEsDf2rt3r9q1aycfHx9ZLBYtWrTomvZ/6NAhWSwWxcXFXdN+b2T33HOP7rnnnqIeBgBcFYUkcAPYv3+//vWvf6lGjRry8PCQt7e3WrRooSlTpuj8+fNOvXdkZKS2bdum119/XR9++KGaNm3q1PtdTz179pTFYpG3t3e+3+PevXtlsVhksVj01ltvFbr/I0eOaNSoUUpMTLwGowWA4qdkUQ8AwN9bunSpHnvsMVmtVvXo0UN169ZVVlaW1q5dqyFDhmjHjh2aNWuWU+59/vx5xcfH66WXXlL//v2dco9q1arp/PnzKlWqlFP6v5qSJUvq3LlzWrx4sR5//HGHa/PmzZOHh4cuXLhg1PeRI0c0evRoVa9eXQ0bNizw577//nuj+wHA9UYhCRRjBw8eVNeuXVWtWjWtXLlSlSpVsl+LiorSvn37tHTpUqfd//jx45IkX19fp93DYrHIw8PDaf1fjdVqVYsWLfTxxx9fVkjOnz9fERER+uKLL67LWM6dO6fSpUvL3d39utwPAP4ppraBYmz8+PHKyMjQ+++/71BE5qlZs6ZeeOEF+88XL17Ua6+9pltvvVVWq1XVq1fXiy++qMzMTIfPVa9eXe3bt9fatWt11113ycPDQzVq1NB///tfe5tRo0apWrVqkqQhQ4bIYrGoevXqki5NCef985+NGjVKFovF4dzy5cvVsmVL+fr6ysvLS7Vq1dKLL75ov36lNZIrV65Uq1atVKZMGfn6+qpDhw7atWtXvvfbt2+fevbsKV9fX/n4+KhXr146d+7clb/Yv+jWrZu+/fZbpaam2s9t2rRJe/fuVbdu3S5rf+rUKQ0ePFj16tWTl5eXvL299eCDD2rLli32NqtWrdKdd94pSerVq5d9ijzvOe+55x7VrVtXCQkJat26tUqXLm3/Xv66RjIyMlIeHh6XPX94eLjKlSunI0eOFPhZAeBaopAEirHFixerRo0auvvuuwvUvk+fPho5cqQaN26sSZMmqU2bNoqNjVXXrl0va7tv3z516dJF999/vyZMmKBy5cqpZ8+e2rFjhySpU6dOmjRpkiTpySef1IcffqjJkycXavw7duxQ+/btlZmZqTFjxmjChAl65JFH9PPPP//t53744QeFh4fr2LFjGjVqlGJiYrRu3Tq1aNFChw4duqz9448/rjNnzig2NlaPP/644uLiNHr06AKPs1OnTrJYLPryyy/t5+bPn6/atWurcePGl7U/cOCAFi1apPbt22vixIkaMmSItm3bpjZt2tiLujp16mjMmDGSpOeee04ffvihPvzwQ7Vu3drez8mTJ/Xggw+qYcOGmjx5stq2bZvv+KZMmaKKFSsqMjJSOTk5kqR3331X33//vd5++20FBQUV+FkB4JqyASiW0tLSbJJsHTp0KFD7xMREmyRbnz59HM4PHjzYJsm2cuVK+7lq1arZJNnWrFljP3fs2DGb1Wq1DRo0yH7u4MGDNkm2N99806HPyMhIW7Vq1S4bw6uvvmr7818rkyZNskmyHT9+/IrjzrvHnDlz7OcaNmxo8/f3t508edJ+bsuWLTY3Nzdbjx49LrvfM88849Dno48+aitfvvwV7/nn5yhTpozNZrPZunTpYrvvvvtsNpvNlpOTYwsMDLSNHj063+/gwoULtpycnMuew2q12saMGWM/t2nTpsueLU+bNm1skmwzZ87M91qbNm0czi1btswmyTZ27FjbgQMHbF5eXraOHTte9RkBwJlIJIFiKj09XZJUtmzZArX/5ptvJEkxMTEO5wcNGiRJl62lDAkJUatWrew/V6xYUbVq1dKBAweMx/xXeWsrv/rqK+Xm5hboM0ePHlViYqJ69uwpPz8/+/n69evr/vvvtz/nn/Xt29fh51atWunkyZP277AgunXrplWrVik5OVkrV65UcnJyvtPa0qV1lW5ul/76zMnJ0cmTJ+3T9ps3by7wPa1Wq3r16lWgtu3atdO//vUvjRkzRp06dZKHh4fefffdAt8LAJyBQhIopry9vSVJZ86cKVD73377TW5ubqpZs6bD+cDAQPn6+uq3335zOF+1atXL+ihXrpxOnz5tOOLLPfHEE2rRooX69OmjgIAAde3aVQsWLPjbojJvnLVq1brsWp06dXTixAmdPXvW4fxfn6VcuXKSVKhneeihh1S2bFl9+umnmjdvnu68887Lvss8ubm5mjRpkm677TZZrVZVqFBBFStW1NatW5WWllbge95yyy2FerHmrbfekp+fnxITEzV16lT5+/sX+LMA4AwUkkAx5e3traCgIG3fvr1Qn/vryy5XUqJEiXzP22w243vkrd/L4+npqTVr1uiHH37Q008/ra1bt+qJJ57Q/ffff1nbf+KfPEseq9WqTp06ae7cuVq4cOEV00hJGjdunGJiYtS6dWt99NFHWrZsmZYvX6477rijwMmrdOn7KYxff/1Vx44dkyRt27atUJ8FAGegkASKsfbt22v//v2Kj4+/attq1aopNzdXe/fudTifkpKi1NRU+xvY10K5cuUc3nDO89fUU5Lc3Nx03333aeLEidq5c6def/11rVy5Uj/++GO+feeNc8+ePZdd2717typUqKAyZcr8swe4gm7duunXX3/VmTNn8n1BKc/nn3+utm3b6v3331fXrl3Vrl07hYWFXfadFLSoL4izZ8+qV69eCgkJ0XPPPafx48dr06ZN16x/ADBBIQkUY0OHDlWZMmXUp08fpaSkXHZ9//79mjJliqRLU7OSLnuzeuLEiZKkiIiIazauW2+9VWlpadq6dav93NGjR7Vw4UKHdqdOnbrss3kbc/91S6I8lSpVUsOGDTV37lyHwmz79u36/vvv7c/pDG3bttVrr72madOmKTAw8IrtSpQocVna+dlnn+mPP/5wOJdX8OZXdBfWsGHDlJSUpLlz52rixImqXr26IiMjr/g9AsD1wIbkQDF26623av78+XriiSdUp04dh99ss27dOn322Wfq2bOnJKlBgwaKjIzUrFmzlJqaqjZt2mjjxo2aO3euOnbseMWtZUx07dpVw4YN06OPPqrnn39e586d04wZM3T77bc7vGwyZswYrVmzRhEREapWrZqOHTum6dOnq3LlymrZsuUV+3/zzTf14IMPKjQ0VL1799b58+f19ttvy8fHR6NGjbpmz/FXbm5uevnll6/arn379hozZox69eqlu+++W9u2bdO8efNUo0YNh3a33nqrfH19NXPmTJUtW1ZlypRRs2bNFBwcXKhxrVy5UtOnT9err75q345ozpw5uueee/TKK69o/PjxheoPAK4VEkmgmHvkkUe0detWdenSRV999ZWioqI0fPhwHTp0SBMmTNDUqVPtbWfPnq3Ro0dr06ZNGjhwoFauXKkRI0bok08+uaZjKl++vBYuXKjSpUtr6NChmjt3rmJjY/Xwww9fNvaqVavqgw8+UFRUlN555x21bt1aK1eulI+PzxX7DwsL03fffafy5ctr5MiReuutt9S8eXP9/PPPhS7CnOHFF1/UoEGDtGzZMr3wwgvavHmzli5dqipVqji0K1WqlObOnasSJUqob9++evLJJ7V69epC3evMmTN65pln1KhRI7300kv2861atdILL7ygCRMmaP369dfkuQCgsCy2wqxGBwAAAP4/EkkAAAAYoZAEAACAEQpJAAAAGKGQBAAAgBEKSQAAABihkAQAAIARCkkAAAAYuSl/s41no/5FPQQATnJ607SiHgIAJ/EowqrEmbXD+V9v3r+3SCQBAABg5KZMJAEAAArFQrZmgkISAADAYinqEdyQKL8BAABghEQSAACAqW0jfGsAAAAwQiIJAADAGkkjJJIAAAAwQiIJAADAGkkjfGsAAAAwQiIJAADAGkkjFJIAAABMbRvhWwMAAIAREkkAAACmto2QSAIAAMAIiSQAAABrJI3wrQEAAMAIiSQAAABrJI2QSAIAAMAIiSQAAABrJI1QSAIAADC1bYTyGwAAAEZIJAEAAJjaNsK3BgAAACMkkgAAACSSRvjWAAAAYIREEgAAwI23tk2QSAIAAMAIiSQAAABrJI1QSAIAALAhuRHKbwAAABghkQQAAGBq2wjfGgAAAIyQSAIAALBG0giJJAAAAIyQSAIAALBG0gjfGgAAAIyQSAIAALBG0giFJAAAAFPbRvjWAAAAYIREEgAAgKltIySSAAAAMEIiCQAAwBpJI3xrAAAAMEIiCQAAwBpJIySSAAAAMEIiCQAAwBpJIxSSAAAAFJJG+NYAAABghEQSAACAl22MkEgCAADACIkkAAAAaySN8K0BAADACIkkAAAAaySNkEgCAADACIkkAAAAaySNUEgCAAAwtW2E8hsAAABGSCQBAIDLs5BIGiGRBAAAgBESSQAA4PJIJM2QSAIAAMAIiSQAAACBpBESSQAAgGLkjz/+0FNPPaXy5cvL09NT9erV0y+//GK/brPZNHLkSFWqVEmenp4KCwvT3r17Hfo4deqUunfvLm9vb/n6+qp3797KyMhwaLN161a1atVKHh4eqlKlisaPH1/osVJIAgAAl2exWJx2FMbp06fVokULlSpVSt9++6127typCRMmqFy5cvY248eP19SpUzVz5kxt2LBBZcqUUXh4uC5cuGBv0717d+3YsUPLly/XkiVLtGbNGj333HP26+np6WrXrp2qVaumhIQEvfnmmxo1apRmzZpVuO/NZrPZCvWJG4Bno/5FPQQATnJ607SiHgIAJ/EowgV3ZZ+Y67S+z3waWeC2w4cP188//6yffvop3+s2m01BQUEaNGiQBg8eLElKS0tTQECA4uLi1LVrV+3atUshISHatGmTmjZtKkn67rvv9NBDD+nw4cMKCgrSjBkz9NJLLyk5OVnu7u72ey9atEi7d+8u8HhJJAEAAJwoMzNT6enpDkdmZma+bb/++ms1bdpUjz32mPz9/dWoUSO999579usHDx5UcnKywsLC7Od8fHzUrFkzxcfHS5Li4+Pl6+trLyIlKSwsTG5ubtqwYYO9TevWre1FpCSFh4drz549On36dIGfjUISAAC4PGdObcfGxsrHx8fhiI2NzXccBw4c0IwZM3Tbbbdp2bJl6tevn55//nnNnXspMU1OTpYkBQQEOHwuICDAfi05OVn+/v4O10uWLCk/Pz+HNvn18ed7FARvbQMAADjRiBEjFBMT43DOarXm2zY3N1dNmzbVuHHjJEmNGjXS9u3bNXPmTEVGFnyK/HohkQQAAC7PmYmk1WqVt7e3w3GlQrJSpUoKCQlxOFenTh0lJSVJkgIDAyVJKSkpDm1SUlLs1wIDA3Xs2DGH6xcvXtSpU6cc2uTXx5/vURAUkgAAAMVEixYttGfPHodz//vf/1StWjVJUnBwsAIDA7VixQr79fT0dG3YsEGhoaGSpNDQUKWmpiohIcHeZuXKlcrNzVWzZs3sbdasWaPs7Gx7m+XLl6tWrVoOb4hfDYUkAACAxYlHIURHR2v9+vUaN26c9u3bp/nz52vWrFmKioq6NEyLRQMHDtTYsWP19ddfa9u2berRo4eCgoLUsWNHSZcSzAceeEDPPvusNm7cqJ9//ln9+/dX165dFRQUJEnq1q2b3N3d1bt3b+3YsUOffvqppkyZctkU/NWwRhIAAKCYuPPOO7Vw4UKNGDFCY8aMUXBwsCZPnqzu3bvb2wwdOlRnz57Vc889p9TUVLVs2VLfffedPDw87G3mzZun/v3767777pObm5s6d+6sqVOn2q/7+Pjo+++/V1RUlJo0aaIKFSpo5MiRDntNFgT7SAK4obCPJHDzKsp9JH27f+S0vlPnPeW0vosaU9sAAAAwwtQ2AABweYX9VYa4hEISAAC4PApJM0xtAwAAwAiJJAAAcHkkkmZIJAEAAGCERBIAAIBA0giJJAAAAIyQSAIAAJfHGkkzJJIAAAAwQiIJAABcHomkGQpJAADg8igkzTC1DQAAACMkkgAAAASSRkgkAQAAYIREEgAAuDzWSJohkQQAAIAREkkAAODySCTNkEgCAADACIkkAABweSSSZigkAQCAy6OQNMPUNgAAAIyQSAIAABBIGiGRBAAAgBESSQAA4PJYI2mGRBIAAABGSCQBAIDLI5E0QyIJAAAAIySSAADA5ZFImqGQBAAAoI40wtQ2AAAAjJBIAgAAl8fUthkSSQAAABghkQQAAC6PRNIMiSQAAACMkEiiWAiq6KOxL3RQuxZ3qLRHKe3//YT+Neojbd6ZJEny9yursS90UFhoHfl4eWrt5n2KGf+Z9icdt/fx9ktddW+zWqpU0UcZ5zO1fstBvTzlK/3vUIokyc+njOa8Hql6t98iP5/SOn4qQ0tWbdXIaYt15uyFInluwBWlpKRo8sQ39fNPP+nChfOqUrWaxowdpzvq1lN2dramTZ2stT+t0eHDv6usl5eahd6tF6IHyd8/QJK0aeMG9enVI9++533ymerWq389Hwc3CRJJMxSSKHK+ZT21Mi5GqzftVcf+03X8dIZqVq2o0+nn7G0WTHpO2Rdz9NjAd5V+9oKef+pefTNzgBp1GqtzF7IkSb/u+l2ffLtJvx89LT+f0nqpb4SWTI9S7favKjfXptzcXC1ZvVWjpy/RidNnVKNKRU0e/rje9imjni/GFdHTA64lPS1NPZ96Uk3vaqZ3Zr6ncn7llPTbb/L29pEkXbhwQbt37dRzffupVq3aSk9P139iX9cL/fvp4wVfSpIaNmykFavWOvT7zttTtGFDvO6oW++6PxPgyiw2m81W1IO41jwb9S/qIaAQXnv+EYU2qKGw3pPzvV6zqr+2fTVSjTuP1a4DyZIu/ZfjoR/G6dVpXytuYXy+n6t7W5A2LXhRIQ+P0sHDJ/Jt8+8n2yi6R5hue/CVa/IscL7Tm6YV9RDwD0ye+JYSf92suA/nF/gz27dtVfeuj+m75T+qUlDQZdezs7N1/72t9WS3p/SvflHXcri4zjyKMN4KHrjUaX0fnBzhtL6LWpEmkidOnNAHH3yg+Ph4JSdfKhACAwN19913q2fPnqpYsWJRDg/XSUSbevph3S7NG/+MWja5TUeOpWrWgp80Z+E6SZLV/dK/pheyLto/Y7PZlJV1UXc3vDXfQrK0h7t6PNJcBw+f0OHk0/net1JFH3W4t6F+StjrhKcCkJ/VP67U3S1aanD08/rll03y9w/QE127qfNjj1/xMxkZGbJYLCrr7X3FPtNSU9Xx0c7OGjZcATPbRorsZZtNmzbp9ttv19SpU+Xj46PWrVurdevW8vHx0dSpU1W7dm398ssvV+0nMzNT6enpDoctN+c6PAGuleBbKujZx1ppX9JxPfLvd/TeZ2s1YWgXdX+4mSRpz6FkJR09pdcGPCLfsp4qVbKEBvUMU+XAcgqs4OPQ13OPtdLxnyfoZPxEtWsRooh+05R90fHfh7mxPXVy3UQd+P51pZ+9oH5jCp6MAPhnDh/+XQs+/VhVq1XXjFnv6/EnntR/Ysfq60UL822fmZmpyRPf0oMPRcjLyyvfNgu//Fx3t2ipgMBAZw4dQD6KbGq7efPmatCggWbOnHnZAlebzaa+fftq69atio/Pf9oyz6hRozR69GiHcyUC7lSpSndd8zHDOdI2TtbmnUlq23Oi/dyEoV3U5I5quidygiSpUZ0qmvFqdzWoVVkXL+Zo5YY9yrXZZLFIHfvPsH/O28tDFf3KKrCCtwb2CFNQRR/d22uiMv+UZgaULyufsqV1WzV/jRnwiH5K2KuBsQuu3wPjH2Fq+8bWpEFd3VG3rv477xP7uTfGjdWO7dv04fxPHdpmZ2dr0MABSklJ0ftxH+ZbSKYkJ+uB+9vqzQmTFdYu3Onjh3MV5dR2jZhvnNb3gYkPOa3volZkieSWLVsUHR2d71tSFotF0dHRSkxMvGo/I0aMUFpamsNRMqCJE0YMZ0k+kW5f+5hn98FkVQksZ//5112/q3nXNxTQarCC272kDv2nq7xPGR08fNLhc+kZF7Q/6bh+3rxf3QbPVq3gAHW4t4FDm5STZ/S/QylaunqbBoz9WP96vLUCK+Q/ZQbg2qpYsaJq3Hqrw7kaNWro6NEjDueys7M1ZNBAHT1yRO/O/uCKaeSihV/Ix9dXbdre67QxA7iyIqv9AwMDtXHjRtWuXTvf6xs3blRAQMBV+7FarbJarQ7nLG4lrskYcX3EJx7Q7dX8Hc7dVtVfSUdPXdY2PePSNj23Vq2oxiFVNXr6kiv2a7FYZJFF7qWu/K+5xe3Sf8j8XRsA107DRo116OBBh3O/HTqkoKBb7D/nFZFJv/2m2XP+K1/fcn/tRtKl2auvFn2phx/pqFKlSjl13Lj5sf2PmSL7f8/BgwfrueeeU0JCgu677z570ZiSkqIVK1bovffe01tvvVVUw8N19PZHK/Vj3CANeaadvli+WXfeUV3PdG6h/q99bG/TKayRjp/O0O/Jp1T3tiC9NaSLFq/aqhXrd0uSqt9SXl3Cm2hF/C6dOJ2hWwJ8NahXO53PzNaytTskSeEtQ+Tv562EHb8p41ymQm6tpHHRHbXu1/35Fq0Arr2nekQq8qknNXvWTLULf1Dbt23V558v0MhRYyRdKiIHRz+vXbt26u133lVuTo5OHL+0X6yPj49Kubvb+9q4Yb3+OHxYnTp3KZJnAVDE2/98+umnmjRpkhISEpSTc+mFiBIlSqhJkyaKiYnR449f+S2+v8P2PzeeB1vV1ZgBj6hm1Yo69MdJTf1opf2tben/tunxL19WySfSNW/JBsXO+s7+Ik2lij6aPrKbGtWponLepXXs5Bmt3bxP42Z9q72/HZMktW56m0b3f1i1awTKWqqkDqek6quViXrrg+VKyzhfJM+NwmON5I1v9aofNXXyRCX9dki3VK6sp3v0sr+1/ccfh/VQu/vy/dzsOf/VnXc1s/88fMggHT3yh+b+ab0lbmxFuUay5uBvndb3vrcedFrfRa1Y7COZnZ2tEycu7fNXoUKFfzxFQSEJ3LwoJIGbF4XkjadYLAwrVaqUKlWqVNTDAAAALoo1kmaKRSEJAABQlKgjzRTZ9j8AAAC4sZFIAgAAl8fUthkSSQAAABghkQQAAC6PQNIMiSQAAACMkEgCAACX5+ZGJGmCRBIAAABGSCQBAIDLY42kGQpJAADg8tj+xwxT2wAAADBCIgkAAFwegaQZEkkAAAAYIZEEAAAujzWSZkgkAQAAYIRCEgAAuDyLxeK0ozBGjRp12edr165tv37hwgVFRUWpfPny8vLyUufOnZWSkuLQR1JSkiIiIlS6dGn5+/tryJAhunjxokObVatWqXHjxrJarapZs6bi4uKMvjcKSQAAgGLkjjvu0NGjR+3H2rVr7deio6O1ePFiffbZZ1q9erWOHDmiTp062a/n5OQoIiJCWVlZWrdunebOnau4uDiNHDnS3ubgwYOKiIhQ27ZtlZiYqIEDB6pPnz5atmxZocfKGkkAAODyitMSyZIlSyowMPCy82lpaXr//fc1f/583XvvvZKkOXPmqE6dOlq/fr2aN2+u77//Xjt37tQPP/yggIAANWzYUK+99pqGDRumUaNGyd3dXTNnzlRwcLAmTJggSapTp47Wrl2rSZMmKTw8vFBjJZEEAAAuz5lT25mZmUpPT3c4MjMzrziWvXv3KigoSDVq1FD37t2VlJQkSUpISFB2drbCwsLsbWvXrq2qVasqPj5ekhQfH6969eopICDA3iY8PFzp6enasWOHvc2f+8hrk9dHYVBIAgAAOFFsbKx8fHwcjtjY2HzbNmvWTHFxcfruu+80Y8YMHTx4UK1atdKZM2eUnJwsd3d3+fr6OnwmICBAycnJkqTk5GSHIjLvet61v2uTnp6u8+fPF+rZmNoGAAAuz5lT2yOGj1BMTIzDOavVmm/bBx980P7P9evXV7NmzVStWjUtWLBAnp6ezhukIRJJAAAAJ7JarfL29nY4rlRI/pWvr69uv/127du3T4GBgcrKylJqaqpDm5SUFPuaysDAwMve4s77+WptvL29C12sUkgCAACXV1y2//mrjIwM7d+/X5UqVVKTJk1UqlQprVixwn59z549SkpKUmhoqCQpNDRU27Zt07Fjx+xtli9fLm9vb4WEhNjb/LmPvDZ5fRQGhSQAAEAxMXjwYK1evVqHDh3SunXr9Oijj6pEiRJ68skn5ePjo969eysmJkY//vijEhIS1KtXL4WGhqp58+aSpHbt2ikkJERPP/20tmzZomXLlunll19WVFSUPQXt27evDhw4oKFDh2r37t2aPn26FixYoOjo6EKPlzWSAADA5RWX7X8OHz6sJ598UidPnlTFihXVsmVLrV+/XhUrVpQkTZo0SW5uburcubMyMzMVHh6u6dOn2z9fokQJLVmyRP369VNoaKjKlCmjyMhIjRkzxt4mODhYS5cuVXR0tKZMmaLKlStr9uzZhd76R5IsNpvN9s8fu3jxbNS/qIcAwElOb5pW1EMA4CQeRRhvNR37o9P6/uXltk7ru6iRSAIAAJf3T9cyuirWSAIAAMAIiSQAAHB5BJJmKCQBAIDLY2rbDFPbAAAAMEIiCQAAXB6BpBkSSQAAABghkQQAAC6PNZJmSCQBAABghEQSAAC4PAJJMySSAAAAMEIiCQAAXB5rJM1QSAIAAJdHHWmGqW0AAAAYIZEEAAAuj6ltMySSAAAAMEIiCQAAXB6JpBkSSQAAABghkQQAAC6PQNIMiSQAAACMkEgCAACXxxpJMxSSAADA5VFHmmFqGwAAAEZIJAEAgMtjatsMiSQAAACMkEgCAACXRyBphkQSAAAARkgkAQCAy3MjkjRCIgkAAAAjJJIAAMDlEUiaoZAEAAAuj+1/zDC1DQAAACMkkgAAwOW5EUgaIZEEAACAERJJAADg8lgjaYZEEgAAAEZIJAEAgMsjkDRDIgkAAAAjJJIAAMDlWUQkaYJCEgAAuDy2/zHD1DYAAACMkEgCAACXx/Y/ZkgkAQAAYIREEgAAuDwCSTMkkgAAADBCIgkAAFyeG5GkERJJAAAAGCGRBAAALo9A0gyFJAAAcHls/2OGqW0AAAAYIZEEAAAuj0DSDIkkAAAAjJBIAgAAl8f2P2ZIJAEAAGCERBIAALg88kgzJJIAAAAwQiIJAABcHvtImqGQBAAALs+NOtIIU9sAAAAwQiIJAABcHlPbZkgkAQAAYIREEgAAuDwCSTMkkgAAADBCIQkAAFyexWJx2vFPvPHGG7JYLBo4cKD93IULFxQVFaXy5cvLy8tLnTt3VkpKisPnkpKSFBERodKlS8vf319DhgzRxYsXHdqsWrVKjRs3ltVqVc2aNRUXF1fo8VFIAgAAFEObNm3Su+++q/r16zucj46O1uLFi/XZZ59p9erVOnLkiDp16mS/npOTo4iICGVlZWndunWaO3eu4uLiNHLkSHubgwcPKiIiQm3btlViYqIGDhyoPn36aNmyZYUaI4UkAABweW4W5x0mMjIy1L17d7333nsqV66c/XxaWpref/99TZw4Uffee6+aNGmiOXPmaN26dVq/fr0k6fvvv9fOnTv10UcfqWHDhnrwwQf12muv6Z133lFWVpYkaebMmQoODtaECRNUp04d9e/fX126dNGkSZMK972ZPR4AAMDNw5lT25mZmUpPT3c4MjMz/3Y8UVFRioiIUFhYmMP5hIQEZWdnO5yvXbu2qlatqvj4eElSfHy86tWrp4CAAHub8PBwpaena8eOHfY2f+07PDzc3kdBUUgCAAA4UWxsrHx8fByO2NjYK7b/5JNPtHnz5nzbJCcny93dXb6+vg7nAwIClJycbG/z5yIy73retb9rk56ervPnzxf42dj+BwAAuDxn7v4zYsQIxcTEOJyzWq35tv3999/1wgsvaPny5fLw8HDiqK4NEkkAAAAnslqt8vb2djiuVEgmJCTo2LFjaty4sUqWLKmSJUtq9erVmjp1qkqWLKmAgABlZWUpNTXV4XMpKSkKDAyUJAUGBl72Fnfez1dr4+3tLU9PzwI/m1Eh+dNPP+mpp55SaGio/vjjD0nShx9+qLVr15p0BwAAUKTcLBanHYVx3333adu2bUpMTLQfTZs2Vffu3e3/XKpUKa1YscL+mT179igpKUmhoaGSpNDQUG3btk3Hjh2zt1m+fLm8vb0VEhJib/PnPvLa5PVR4O+tUK0lffHFFwoPD5enp6d+/fVX+2LRtLQ0jRs3rrDdAQAA4P8rW7as6tat63CUKVNG5cuXV926deXj46PevXsrJiZGP/74oxISEtSrVy+FhoaqefPmkqR27dopJCRETz/9tLZs2aJly5bp5ZdfVlRUlD0J7du3rw4cOKChQ4dq9+7dmj59uhYsWKDo6OhCjbfQheTYsWM1c+ZMvffeeypVqpT9fIsWLbR58+bCdgcAAFDkLBbnHdfapEmT1L59e3Xu3FmtW7dWYGCgvvzyS/v1EiVKaMmSJSpRooRCQ0P11FNPqUePHhozZoy9TXBwsJYuXarly5erQYMGmjBhgmbPnq3w8PBCjcVis9lshflA6dKltXPnTlWvXl1ly5bVli1bVKNGDR04cEAhISG6cOFCoQbgDJ6N+hf1EAA4yelN04p6CACcxKMIXwF+dsF2p/X93uN1ndZ3USt0IhkYGKh9+/Zddn7t2rWqUaPGNRkUAADA9VRcf0VicVfoQvLZZ5/VCy+8oA0bNshisejIkSOaN2+eBg8erH79+jljjAAAACiGCh0iDx8+XLm5ubrvvvt07tw5tW7dWlarVYMHD9aAAQOcMUYAAACnusmDQ6cpdCFpsVj00ksvaciQIdq3b58yMjIUEhIiLy8vZ4wPAADA6Qq7TQ8uMV7W6u7ubt+LCAAAAK6n0IVk27Zt/3bh6MqVK//RgAAAAK43AkkzhS4kGzZs6PBzdna2EhMTtX37dkVGRl6rcQEAAKCYK3QhOWnSpHzPjxo1ShkZGf94QAAAANfbzb5Nj7MY/a7t/Dz11FP64IMPrlV3AAAAKOau2R7y8fHx8vDwuFbd/SPH1k8t6iEAcJK0c9lFPQQATuLhXerqjZzkmiVrLqbQhWSnTp0cfrbZbDp69Kh++eUXvfLKK9dsYAAAACjeCl1I+vj4OPzs5uamWrVqacyYMWrXrt01GxgAAMD1whpJM4UqJHNyctSrVy/Vq1dP5cqVc9aYAAAAris36kgjhVoSUKJECbVr106pqalOGg4AAABuFIVeW1q3bl0dOHDAGWMBAAAoEm4W5x03s0IXkmPHjtXgwYO1ZMkSHT16VOnp6Q4HAAAAXEOB10iOGTNGgwYN0kMPPSRJeuSRRxwWptpsNlksFuXk5Fz7UQIAADgRL9uYKXAhOXr0aPXt21c//vijM8cDAACAG0SBC0mbzSZJatOmjdMGAwAAUBRu9rWMzlKoNZLEvgAAAMhTqH0kb7/99qsWk6dOnfpHAwIAALjeyMrMFKqQHD169GW/2QYAAOBG50YlaaRQhWTXrl3l7+/vrLEAAADgBlLgQpL1kQAA4GZV6I21IakQ31veW9sAAACAVIhEMjc315njAAAAKDJMvJohyQUAAICRQr1sAwAAcDPirW0zJJIAAAAwQiIJAABcHoGkGQpJAADg8vhd22aY2gYAAIAREkkAAODyeNnGDIkkAAAAjJBIAgAAl0cgaYZEEgAAAEZIJAEAgMvjrW0zJJIAAAAwQiIJAABcnkVEkiYoJAEAgMtjatsMU9sAAAAwQiIJAABcHomkGRJJAAAAGCGRBAAALs/CjuRGSCQBAABghEQSAAC4PNZImiGRBAAAgBESSQAA4PJYImmGQhIAALg8NypJI0xtAwAAwAiJJAAAcHm8bGOGRBIAAABGSCQBAIDLY4mkGRJJAAAAGCGRBAAALs9NRJImSCQBAABghEQSAAC4PNZImqGQBAAALo/tf8wwtQ0AAAAjJJIAAMDl8SsSzZBIAgAAwAiJJAAAcHkEkmZIJAEAAIqJGTNmqH79+vL29pa3t7dCQ0P17bff2q9fuHBBUVFRKl++vLy8vNS5c2elpKQ49JGUlKSIiAiVLl1a/v7+GjJkiC5evOjQZtWqVWrcuLGsVqtq1qypuLg4o/FSSAIAAJfnZrE47SiMypUr64033lBCQoJ++eUX3XvvverQoYN27NghSYqOjtbixYv12WefafXq1Tpy5Ig6depk/3xOTo4iIiKUlZWldevWae7cuYqLi9PIkSPtbQ4ePKiIiAi1bdtWiYmJGjhwoPr06aNly5YV+nuz2Gw2W6E/Vcydycwt6iEAcJJzmTlFPQQAThLgXarI7v3+xiSn9d37rqr/6PN+fn5688031aVLF1WsWFHz589Xly5dJEm7d+9WnTp1FB8fr+bNm+vbb79V+/btdeTIEQUEBEiSZs6cqWHDhun48eNyd3fXsGHDtHTpUm3fvt1+j65duyo1NVXfffddocZGIgkAAFyexeK8IzMzU+np6Q5HZmbmVceUk5OjTz75RGfPnlVoaKgSEhKUnZ2tsLAwe5vatWuratWqio+PlyTFx8erXr169iJSksLDw5Wenm5PNePj4x36yGuT10dhUEgCAACX5+bEIzY2Vj4+Pg5HbGzsFceybds2eXl5yWq1qm/fvlq4cKFCQkKUnJwsd3d3+fr6OrQPCAhQcnKyJCk5OdmhiMy7nnft79qkp6fr/PnzBf7OJN7aBgAAcKoRI0YoJibG4ZzVar1i+1q1aikxMVFpaWn6/PPPFRkZqdWrVzt7mEYoJAEAgMuzOHH/H6vV+reF41+5u7urZs2akqQmTZpo06ZNmjJlip544gllZWUpNTXVIZVMSUlRYGCgJCkwMFAbN2506C/vre4/t/nrm94pKSny9vaWp6dnoZ6NqW0AAIBiLDc3V5mZmWrSpIlKlSqlFStW2K/t2bNHSUlJCg0NlSSFhoZq27ZtOnbsmL3N8uXL5e3trZCQEHubP/eR1yavj8IgkQQAAC6vuOxHPmLECD344IOqWrWqzpw5o/nz52vVqlVatmyZfHx81Lt3b8XExMjPz0/e3t4aMGCAQkND1bx5c0lSu3btFBISoqefflrjx49XcnKyXn75ZUVFRdlT0b59+2ratGkaOnSonnnmGa1cuVILFizQ0qVLCz1eCkkAAIBi4tixY+rRo4eOHj0qHx8f1a9fX8uWLdP9998vSZo0aZLc3NzUuXNnZWZmKjw8XNOnT7d/vkSJElqyZIn69eun0NBQlSlTRpGRkRozZoy9TXBwsJYuXaro6GhNmTJFlStX1uzZsxUeHl7o8bKPJIAbCvtIAjevotxH8qOEw07r+6kmlZ3Wd1FjjSQAAACMMLUNAABcXnFZI3mjoZAEAAAuz4m7/9zUmNoGAACAERJJAADg8py5IfnNjEQSAAAARkgkAQCAyyNZM8P3BgAAACMkkgAAwOWxRtIMiSQAAACMkEgCAACXRx5phkQSAAAARkgkAQCAy2ONpBkKSQAA4PKYojXD9wYAAAAjJJIAAMDlMbVthkQSAAAARkgkAQCAyyOPNEMiCQAAACMkkgAAwOWxRNIMiSQAAACMkEgCAACX58YqSSMUkgAAwOUxtW2GqW0AAAAYIZEEAAAuz8LUthESSQAAABghkQQAAC6PNZJmSCQBAABghEQSAAC4PLb/MUMiCQAAACMkkgAAwOWxRtIMhSQAAHB5FJJmmNoGAACAERJJAADg8tiQ3AyJJAAAAIyQSAIAAJfnRiBphEQSAAAARkgkAQCAy2ONpBkSSQAAABghkQQAAC6PfSTNUEgCAACXx9S2Gaa2AQAAYIREEgAAuDy2/zFDIgkAAAAjJJIAAMDlsUbSDIkkAAAAjJBIoth5+IH7dPTIkcvOP/bEkxr20kgd/j1JkyeMV+Kvm5WdlaXQFq00ZMRLKl++gr1t9IB/6397duv0qZMq6+2tu5qH6vmBg1XR3/96Pgrg8hI3/6JPPpyjPbt36uSJ43r9zSlqdc999us2m00fvPuOFi/6XBkZZ1SvfiPFDH9FVapWs7fZs3un3n17onbv3CG3Em5q0/Z+RUUPVenSpSVJ3y5epNgxL+d7/6+WrVY5v/LOfUjcFNj+x4zFZrPZinoQ19qZzNyiHgL+gdOnTiknN8f+8/59exX1XG/NfH+u7rijrrp26ajba9XSv/oNkCTNeGeqjh8/priPPpGb26WQfd6Hcapfv6EqVKyoY8eOacqE8ZKkDz78+Po/EK6pc5k5V2+EYmP9zz9p29ZfVat2iF4eOvCyQnLe3Pc1L262Rox6XUFBt2j2zGk6sG+v/rvgK1mtVp04fkyRXTvq3vsf0GNdn9bZsxl6e+J/VL5CRb32n0mSpMwLF5SRkeFw39jRLykrK1NT3427no+LfyjAu1SR3Xvt3tNO67vlbeWc1ndRI5FEsVPOz8/h57nvv6fKVaqqSdM7tSF+nY4e+UPzFnwpLy8vSdLosbFq27KZNm1cr2bN75YkdX+6p/3zlYJuUeQzz2rwwP66mJ2tkqWK7i8qwNU0b9FKzVu0yveazWbTZx9/qKefeU6t2twrSXpp9Dh1DG+jtatX6L52D2ndT6tVsmRJRQ992f4fioNGjFSvJzvp8O9JqlylqqweHrJ6eNj7TT19Spt/2aBhr4xx/gPipkEgaYY1kijWsrOz9M3SxXqkYydZLBZlZWXJYrHI3d3d3sbdapWbm5sSN2/Ot4+0tFR9981i1W/YiCISKEaO/nFYp06eUNO7Qu3nvLzKqs4d9bV96xZJl/4OKFmylL2IlCSr9VLRuC0x/z/z3y39Wh4enrrn3nZOHD1uNm4Wi9OOm1mxLiR///13PfPMM3/bJjMzU+np6Q5HZmbmdRohnG3VyhXKOHNGD3d4VJJUr34DeXh66u1Jb+nC+fM6f+6cJk8Yr5ycHJ04cdzhs1MnvaWWdzXWfa1ClXz0qCZMmVYUjwDgCk6ePCFJKlfecQ2jX/nyOvX/rzVu2kynTp7Uxx9+oOzsbJ1JT9O70y5NaZ/8y5/5PEu//lJh4Q85pJQAnKNYF5KnTp3S3Llz/7ZNbGysfHx8HI4J49+4TiOEs3218Avd3aKV/SWZcn5++s9bk7Vm9Sq1at5E97S4S2fOpKt2nZDL/quvR8/emrfgC017d7bcSpTQqy8N1024JBi4qQXfWlMvjnpdn340V+1aNVXHB+5RpaBb5OdXXha3y/8vbPvWRP128IAiOnQqgtHiRmZx4nEzK9I1kl9//fXfXj9w4MBV+xgxYoRiYmIczmWJ6cubwdEjf2jj+niNnzTV4Xzzu1voq2++V+rp0ypRooTKensrvG0r3VK5ikM733Ll5FuunKpVD1Zw8K2KaNdW27Ymqn6DRtfzMQBcQd5OC6dPnlSFChXt50+dPKmat9ey/3z/AxG6/4EInTp5Qh6epWWxSAvm/1dBt1S+rM8lX32h226vrVp17nD+AwAo2kKyY8eOslgsf5sSWa6ytsBqtcpqtTqc463tm8PXixaqnJ+fWrZqk+9133KX3oLbtGG9Tp06qdb33HvFvmy2S/9OZGVlX/uBAjBS6ZbK8itfQQmb1uu2WrUlSWczMrRrx1Z17PL4Ze39/n/hufTrL+XublXTZqEO18+dO6cff1im56IGOn3suAnd7NGhkxRpIVmpUiVNnz5dHTp0yPd6YmKimjRpcp1HheIgNzdXi7/6Uu0f6aiSJR3/Nf160ZcKDq6hcn5+2rolURP+M07dno5U9eBgSdL2rVu0Y8d2NWzUWN7e3jr8+++a8c5UVa5SVfUbNCyCpwFc17lz5/TH70n2n48e+UN79+yWt4+PAgIr6bEnn9Z/P5ilylWqqdItt+j9mdNUvoK/Wrb5vy2CvlgwX3XrN1Rpz9LatCFeM6ZO0L/6D1TZst4O91q5/Fvl5OSo3YPtr9vzAa6uSAvJJk2aKCEh4YqF5NXSSty8Nq6PV/LRo3qk4+XrnH47dFDvTJmktLQ0Bd0SpF7P9lX3pyPt1z08PPXjD8s1a/rbOn/+vCpUqKjQFi3V+81+Dm97A3C+Pbu264W+//fS5LRJl/Z0fSCig14c9bq69XhGF86f11vjRl3akLxBY701dabDTNPuHds0Z9Y7On/unKpWD9bgF0cq/KFHLrvX0q++VOt7wi4rMIGC4FckminSDcl/+uknnT17Vg888EC+18+ePatffvlFbdrkP7V5JUxtAzcvNiQHbl5FuSH5hv1pTuu72a0+Tuu7qPGbbQDcUCgkgZtXURaSGw84r5C8q8bNW0jym20AAIDLY2LbTLHeRxIAAADFF4kkAAAAkaQREkkAAAAYoZAEAAAuz+LE/xVGbGys7rzzTpUtW1b+/v7q2LGj9uzZ49DmwoULioqKUvny5eXl5aXOnTsrJSXFoU1SUpIiIiJUunRp+fv7a8iQIbp48aJDm1WrVqlx48ayWq2qWbOm4uLiCv29UUgCAAAUE6tXr1ZUVJTWr1+v5cuXKzs7W+3atdPZs2ftbaKjo7V48WJ99tlnWr16tY4cOaJOnf5v3+WcnBxFREQoKytL69at09y5cxUXF6eRI0fa2xw8eFARERFq27atEhMTNXDgQPXp00fLli0r1HjZ/gfADYXtf4CbV1Fu/5NwKN1pfTepbr5J/vHjx+Xv76/Vq1erdevWSktLU8WKFTV//nx16dJFkrR7927VqVNH8fHxat68ub799lu1b99eR44cUUBAgCRp5syZGjZsmI4fPy53d3cNGzZMS5cu1fbt2+336tq1q1JTU/Xdd98VeHwkkgAAAE6UmZmp9PR0hyMzM7NAn01Lu7S/pZ+fnyQpISFB2dnZCgsLs7epXbu2qlatqvj4eElSfHy86tWrZy8iJSk8PFzp6enasWOHvc2f+8hrk9dHQVFIAgAAl2dx4hEbGysfHx+HIzY29qpjys3N1cCBA9WiRQvVrVtXkpScnCx3d3f5+vo6tA0ICFBycrK9zZ+LyLzredf+rk16errOnz9/1bHlYfsfAAAAJ27/M2LECMXExDic+/Pvk7+SqKgobd++XWvXrnXW0P4xCkkAAAAnslqtBSoc/6x///5asmSJ1qxZo8qVK9vPBwYGKisrS6mpqQ6pZEpKigIDA+1tNm7c6NBf3lvdf27z1ze9U1JS5O3tLU9PzwKPk6ltAADg8orL9j82m039+/fXwoULtXLlSgUHBztcb9KkiUqVKqUVK1bYz+3Zs0dJSUkKDQ2VJIWGhmrbtm06duyYvc3y5cvl7e2tkJAQe5s/95HXJq+PAn9vvLUN4EbCW9vAzaso39r+9bczTuu7UbWyBW7773//W/Pnz9dXX32lWrVq2c/7+PjYk8J+/frpm2++UVxcnLy9vTVgwABJ0rp16yRd2v6nYcOGCgoK0vjx45WcnKynn35affr00bhx4yRd2v6nbt26ioqK0jPPPKOVK1fq+eef19KlSxUeHl7g8VJIArihUEgCN6+iLCQTk5xXSDasWvBC0mLJP8GcM2eOevbsKenShuSDBg3Sxx9/rMzMTIWHh2v69On2aWtJ+u2339SvXz+tWrVKZcqUUWRkpN544w2VLPl/qxpXrVql6Oho7dy5U5UrV9Yrr7xiv0eBx0shCeBGQiEJ3LwoJG88vGwDAABcnhNf2r6p8bINAAAAjJBIAgAAEEkaoZAEAAAur7Db9OASprYBAABghEQSAAC4vCvsuoOrIJEEAACAERJJAADg8ggkzZBIAgAAwAiJJAAAAJGkERJJAAAAGCGRBAAALo99JM2QSAIAAMAIiSQAAHB57CNphkISAAC4POpIM0xtAwAAwAiJJAAAAJGkERJJAAAAGCGRBAAALo/tf8yQSAIAAMAIiSQAAHB5bP9jhkQSAAAARkgkAQCAyyOQNEMhCQAAQCVphKltAAAAGCGRBAAALo/tf8yQSAIAAMAIiSQAAHB5bP9jhkQSAAAARkgkAQCAyyOQNEMiCQAAACMkkgAAAESSRigkAQCAy2P7HzNMbQMAAMAIiSQAAHB5bP9jhkQSAAAARkgkAQCAyyOQNEMiCQAAACMkkgAAAESSRkgkAQAAYIREEgAAuDz2kTRDIQkAAFwe2/+YYWobAAAARkgkAQCAyyOQNEMiCQAAACMkkgAAwOWxRtIMiSQAAACMkEgCAACwStIIiSQAAACMkEgCAACXxxpJMxSSAADA5VFHmmFqGwAAAEZIJAEAgMtjatsMiSQAAACMkEgCAACXZ2GVpBESSQAAABghkQQAACCQNEIiCQAAACMkkgAAwOURSJqhkAQAAC6P7X/MMLUNAAAAIySSAADA5bH9jxkSSQAAABihkAQAALA48SikNWvW6OGHH1ZQUJAsFosWLVrkcN1ms2nkyJGqVKmSPD09FRYWpr179zq0OXXqlLp37y5vb2/5+vqqd+/eysjIcGizdetWtWrVSh4eHqpSpYrGjx9f6LFSSAIAABQjZ8+eVYMGDfTOO+/ke338+PGaOnWqZs6cqQ0bNqhMmTIKDw/XhQsX7G26d++uHTt2aPny5VqyZInWrFmj5557zn49PT1d7dq1U7Vq1ZSQkKA333xTo0aN0qxZswo1VovNZrOZPWbxdSYzt6iHAMBJzmXmFPUQADhJgHepIrv3iYyLTuu7bKkcZWZmOpyzWq2yWq1X/azFYtHChQvVsWNHSZfSyKCgIA0aNEiDBw+WJKWlpSkgIEBxcXHq2rWrdu3apZCQEG3atElNmzaVJH333Xd66KGHdPjwYQUFBWnGjBl66aWXlJycLHd3d0nS8OHDtWjRIu3evbvAz0YiCQAA4ESxsbHy8fFxOGJjY436OnjwoJKTkxUWFmY/5+Pjo2bNmik+Pl6SFB8fL19fX3sRKUlhYWFyc3PThg0b7G1at25tLyIlKTw8XHv27NHp06cLPB7e2gYAAC7PmftIjhgxQjExMQ7nCpJG5ic5OVmSFBAQ4HA+ICDAfi05OVn+/v4O10uWLCk/Pz+HNsHBwZf1kXetXLlyBRoPhSQAAHB5ztz+p6DT2DciprYBAABuEIGBgZKklJQUh/MpKSn2a4GBgTp27JjD9YsXL+rUqVMObfLr48/3KAgKSQAA4PIsFucd11JwcLACAwO1YsUK+7n09HRt2LBBoaGhkqTQ0FClpqYqISHB3mblypXKzc1Vs2bN7G3WrFmj7Oxse5vly5erVq1aBZ7WligkAQAAipWMjAwlJiYqMTFR0qUXbBITE5WUlCSLxaKBAwdq7Nix+vrrr7Vt2zb16NFDQUFB9je769SpowceeEDPPvusNm7cqJ9//ln9+/dX165dFRQUJEnq1q2b3N3d1bt3b+3YsUOffvqppkyZctlazqth+x8ANxS2/wFuXkW5/c/pc877u6Vc6RKFar9q1Sq1bdv2svORkZGKi4uTzWbTq6++qlmzZik1NVUtW7bU9OnTdfvtt9vbnjp1Sv3799fixYvl5uamzp07a+rUqfLy8rK32bp1q6KiorRp0yZVqFBBAwYM0LBhwwo1VgpJADcUCkng5kUheePhrW0AAODynLn9z82MNZIAAAAwQiIJAABcnjP3kbyZUUgCAACXx9S2Gaa2AQAAYIREEgAAuDwCSTMkkgAAADBCIgkAAEAkaYREEgAAAEZIJAEAgMtj+x8zJJIAAAAwQiIJAABcHvtImiGRBAAAgBESSQAA4PIIJM1QSAIAAFBJGmFqGwAAAEZIJAEAgMtj+x8zJJIAAAAwQiIJAABcHtv/mCGRBAAAgBGLzWazFfUgAFOZmZmKjY3ViBEjZLVai3o4AK4h/nwDxR+FJG5o6enp8vHxUVpamry9vYt6OACuIf58A8UfU9sAAAAwQiEJAAAAIxSSAAAAMEIhiRua1WrVq6++ykJ84CbEn2+g+ONlGwAAABghkQQAAIARCkkAAAAYoZAEAACAEQpJAAAAGKGQxA3tnXfeUfXq1eXh4aFmzZpp48aNRT0kAP/QmjVr9PDDDysoKEgWi0WLFi0q6iEBuAIKSdywPv30U8XExOjVV1/V5s2b1aBBA4WHh+vYsWNFPTQA/8DZs2fVoEEDvfPOO0U9FABXwfY/uGE1a9ZMd955p6ZNmyZJys3NVZUqVTRgwAANHz68iEcH4FqwWCxauHChOnbsWNRDAZAPEknckLKyspSQkKCwsDD7OTc3N4WFhSk+Pr4IRwYAgOugkMQN6cSJE8rJyVFAQIDD+YCAACUnJxfRqAAAcC0UkgAAADBCIYkbUoUKFVSiRAmlpKQ4nE9JSVFgYGARjQoAANdCIYkbkru7u5o0aaIVK1bYz+Xm5mrFihUKDQ0twpEBAOA6Shb1AABTMTExioyMVNOmTXXXXXdp8uTJOnv2rHr16lXUQwPwD2RkZGjfvn32nw8ePKjExET5+fmpatWqRTgyAH/F9j+4oU2bNk1vvvmmkpOT1bBhQ02dOlXNmjUr6mEB+AdWrVqltm3bXnY+MjJScXFx139AAK6IQhIAAABGWCMJAAAAIxSSAAAAMEIhCQAAACMUkgAAADBCIQkAAAAjFJIAAAAwQiEJAAAAIxSSAAAAMEIhCaDY6tmzpzp27Gj/+Z577tHAgQOv+zhWrVoli8Wi1NTU635vACjOKCQBFFrPnj1lsVhksVjk7u6umjVrasyYMbp48aJT7/vll1/qtddeK1Bbij8AcL6SRT0AADemBx54QHPmzFFmZqa++eYbRUVFqVSpUhoxYoRDu6ysLLm7u1+Te/r5+V2TfgAA1waJJAAjVqtVgYGBqlatmvr166ewsDB9/fXX9uno119/XUFBQapVq5Yk6ffff9fjjz8uX19f+fn5qUOHDjp06JC9v5ycHMXExMjX11fly5fX0KFDZbPZHO7516ntzMxMDRs2TFWqVJHValXNmjX1/vvv69ChQ2rbtq0kqVy5crJYLOrZs6ckKTc3V7GxsQoODpanp6caNGigzz//3OE+33zzjW6//XZ5enqqbdu2DuMEAPwfCkkA14Snp6eysrIkSStWrNCePXu0fPlyLVmyRNnZ2QoPD1fZsmX1008/6eeff5aXl5ceeOAB+2cmTJiguLg4ffDBB1q7dq1OnTqlhQsX/u09e/TooY8//lhTp07Vrl279O6778rLy0tVqlTRF198IUnas2ePjh49qilTpkiSYmNj9d///lczZ87Ujh07FB0draeeekqrV6+WdKng7dSpkx5++GElJiaqT58+Gj58uLO+NgC4oTG1DeAfsdlsWrFihZYtW6YBAwbo+PHjKlOmjGbPnm2f0v7oo4+Um5ur2bNny2KxSJLmzJkjX19frVq1Su3atdPkyZM1YsQIderUSZI0c+ZMLVu27Ir3/d///qcFCxZo+fLlCgsLkyTVqFHDfj1vGtzf31++vr6SLiWY48aN0w8//KDQ0FD7Z9auXat3331Xbdq00YwZM3TrrbdqwoQJkqRatWpp27Zt+s9//nMNvzUAuDlQSAIwsmTJEnl5eSk7O1u5ubnq1q2bRo0apaioKNWrV89hXeSWLVu0b98+lS1b1qGPCxcuaP/+/UpLS9PRo0fVrFkz+7WSJUuqadOml01v50lMTFSJEiXUpk2bAo953759OnfunO6//36H81lZWWrUqJEkadeuXQ7jkGQvOgEAjigkARhp27atZsyYIXd3dwUFBalkyf/766RMmTIObTMyMtSkSRPNmzfvsn4qVqxodH9PT89CfyYjI0OStHTpUt1yyy0O16xWq9E4AMCVUUgCMFKmTBnVrFmzQG0bN26sTz/9VP7+/vL29s63TaVKlbRhwwa1bt1aknTx4kUlJCSocePG+bavV6+ecnNztXr1avvU9p/lJaI5OTn2cyEhIbJarUpKSrpiklmnTh19/fXXDufWr19/9YcEABfEyzYAnK579+6qUKGCOnTooJ9++kkHDx7UqlWr9Pzzz+vw4cOSpBdeeEFvvPGGFi1apN27d+vf//733+4BWb16dUVGRuqZZ57RokWL7H0uWLBAklStWjVZLBYtWbJEx48fV0ZGhsqWLavBgwcrOjpac+fO1f79+7V582a9/fbbmjt3riSpb9++2rt3r4YMGaI9e/Zo/vz5iouLc/ZXBAA3JApJAE5XunRprVmzRlWrVlWnTp1Up04d9e7dWxcuXLAnlIMGDdLTTz+tyMhIhYaGqmzZsnr00Uf/tt8ZM2aoS5cu+ve//63atWvr2Wef1dmzZyVJt9xyi0aPHq3hw4crICBA/fv3lyS99tpreuWVVxQbG6s6derogQce0NKlSxUcHCxJqlq1qr744gstWrRIDRo00MyZMzVu3DgnfjsAcOOy2K60kh0AAAD4GySSAAAAMEIhCQAAACMUkgAAADBCIQkAAAAjFJIAAAAwQiEJAAAAIxSSAAAAMEIhCQAAACMUkgAAADBCIQkAAAAjFJIAAAAw8v8AM4q6tdJOJwUAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}